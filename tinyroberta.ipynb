{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12830809,"sourceType":"datasetVersion","datasetId":8114587}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-22T14:50:32.144268Z","iopub.execute_input":"2025-08-22T14:50:32.144549Z","iopub.status.idle":"2025-08-22T14:50:32.522618Z","shell.execute_reply.started":"2025-08-22T14:50:32.144527Z","shell.execute_reply":"2025-08-22T14:50:32.521351Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/wiki-cc-openweb/wiki_small_50k/wiki_small_50k.csv\n/kaggle/input/wiki-cc-openweb/openweb_50k (2)/openweb_50k.csv\n/kaggle/input/wiki-cc-openweb/cc_news/cc_news.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from datasets import load_dataset, concatenate_datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T14:24:23.192270Z","iopub.execute_input":"2025-08-23T14:24:23.192523Z","iopub.status.idle":"2025-08-23T14:24:26.390951Z","shell.execute_reply.started":"2025-08-23T14:24:23.192496Z","shell.execute_reply":"2025-08-23T14:24:26.390182Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"wiki_ds = load_dataset(\"csv\", data_files=\"/kaggle/input/wiki-cc-openweb/wiki_small_50k/wiki_small_50k.csv\")\nowt_ds = load_dataset(\"csv\", data_files=\"/kaggle/input/wiki-cc-openweb/openweb_50k (2)/openweb_50k.csv\")\ncc_news_ds = load_dataset(\"csv\", data_files=\"/kaggle/input/wiki-cc-openweb/cc_news/cc_news.csv\")\ntrain_ds = concatenate_datasets([wiki_ds[\"train\"],owt_ds[\"train\"],cc_news_ds[\"train\"]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T14:24:26.392694Z","iopub.execute_input":"2025-08-23T14:24:26.393042Z","iopub.status.idle":"2025-08-23T14:24:42.176350Z","shell.execute_reply.started":"2025-08-23T14:24:26.393025Z","shell.execute_reply":"2025-08-23T14:24:42.175817Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38fa202bcea340a6898a62c8e484c63c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef903a76f8a54217939c46371e113823"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b5a1d733a9744e79a19cbf127f0cd67"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"from tokenizers import ByteLevelBPETokenizer\nspecial_tokens = ['<s>','<pad>','</s>','<unk>','<mask>']\ntokenizer = ByteLevelBPETokenizer()\ntokenizer.train_from_iterator([x['text'] for x in train_ds], vocab_size = 10_000,min_frequency =2, special_tokens=special_tokens)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T14:25:10.661933Z","iopub.execute_input":"2025-08-23T14:25:10.662670Z","iopub.status.idle":"2025-08-23T14:27:09.611497Z","shell.execute_reply.started":"2025-08-23T14:25:10.662638Z","shell.execute_reply":"2025-08-23T14:27:09.610689Z"}},"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"tokenizer.save_model(\"/kaggle/working/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T14:27:24.481569Z","iopub.execute_input":"2025-08-23T14:27:24.482192Z","iopub.status.idle":"2025-08-23T14:27:24.493222Z","shell.execute_reply.started":"2025-08-23T14:27:24.482139Z","shell.execute_reply":"2025-08-23T14:27:24.492406Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/vocab.json', '/kaggle/working/merges.txt']"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from transformers import RobertaTokenizer\ntokenizer = RobertaTokenizer.from_pretrained(\"/kaggle/working/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T14:29:23.106791Z","iopub.execute_input":"2025-08-23T14:29:23.107240Z","iopub.status.idle":"2025-08-23T14:29:29.330773Z","shell.execute_reply.started":"2025-08-23T14:29:23.107205Z","shell.execute_reply":"2025-08-23T14:29:29.330011Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"indices = [0, 2]  # Example indices\nsubset = train_ds.select(indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T14:55:41.894197Z","iopub.execute_input":"2025-08-23T14:55:41.894615Z","iopub.status.idle":"2025-08-23T14:55:41.904805Z","shell.execute_reply.started":"2025-08-23T14:55:41.894580Z","shell.execute_reply":"2025-08-23T14:55:41.903913Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"type(subset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T14:55:43.801822Z","iopub.execute_input":"2025-08-23T14:55:43.802446Z","iopub.status.idle":"2025-08-23T14:55:43.807308Z","shell.execute_reply.started":"2025-08-23T14:55:43.802420Z","shell.execute_reply":"2025-08-23T14:55:43.806525Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"datasets.arrow_dataset.Dataset"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"def tokenize(batch):\n    result = tokenizer(batch[\"text\"], truncation=True, max_length=254,return_tensors=\"pt\" )\n    print(result)\n    result = {\"input_ids\": result[\"input_ids\"].squeeze(0),\n    \"attention_mask\":result[\"attention_mask\"].squeeze(0)}\n    print(result)\n    return result\n\ntrain_dataset = subset.map(tokenize, remove_columns=[\"text\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:02:18.283107Z","iopub.execute_input":"2025-08-23T15:02:18.283400Z","iopub.status.idle":"2025-08-23T15:02:18.483442Z","shell.execute_reply.started":"2025-08-23T15:02:18.283379Z","shell.execute_reply":"2025-08-23T15:02:18.482668Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a61457829fe406f9764e43828261379"}},"metadata":{}},{"name":"stdout","text":"{'input_ids': tensor([[   0, 9634, 2278, 4204, 2374, 7303, 2278, 4204,  365,  809, 1270,  836,\n         5215,  688, 1275, 1939, 2763,   25,   13,  352,  368, 2743, 2875,  400,\n           18,  203,  203, 7994,  203, 1238, 7102,  333,  567,  559,  353, 9225,\n          784,   83,  348,   77, 1486,   77,   16,  333, 4428,  347, 1171,  325,\n         1325,  365,   88,  875,  444, 1274,  288, 1350, 7470,  660, 1309, 3409,\n          729,  295,  352,  444, 6709,  601,  286,  457,  303,   18,  631, 1743,\n          327,  291, 4820,  376,  267, 4284, 4826,  290, 4193,   16, 4284, 2721,\n          290, 4193,  295, 2271,  409, 2721,   16, 7418,   31,  444, 2131, 2533,\n          373,  309,  828, 3553, 4864,  295, 1405,  271,   86,  499, 1754, 6139,\n           16,  515, 1141, 1720, 7248, 4725,  353, 7303, 2102, 4849,   18,  631,\n         1791,  353, 1904,  264,  400, 5550, 1057,  603,  335,   16,  295, 4802,\n          384, 2435, 5256,   83,  353, 1904,  264,  400,  315,  363, 1777, 8506,\n         2655,   77,  295,  286,  488,  400,  315, 1045,   69, 4586, 3584,   18,\n          631, 1074,  749,  348,   77, 1486,   77,  409, 3875,   86, 1076,   16,\n          353, 2710,  510,  355,  291,  787,  444, 1342, 6038, 6009,  784,  686,\n          291,  343, 1791,   18,  203,  203, 2025, 1852,  203, 2374, 2278, 4204,\n          203,  384,  540, 1660,  266,   71, 5870,  295,  267, 5578, 4420,  412,\n          512,   30,  836,  809,   17,  706,  809,   30, 2074, 2337,  713,  315,\n         4068, 2999,  290, 4800, 1217,  314, 5870, 2020,  316, 1978,  327, 5578,\n         4420, 1195, 5830,  509,  295,  315,  390, 3716,  813,  203,  203, 1257,\n         5215, 3014,  203,  706, 3589, 3578,  203,  809,  380,   17, 3199, 1446,\n          595,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n{'input_ids': tensor([   0, 9634, 2278, 4204, 2374, 7303, 2278, 4204,  365,  809, 1270,  836,\n        5215,  688, 1275, 1939, 2763,   25,   13,  352,  368, 2743, 2875,  400,\n          18,  203,  203, 7994,  203, 1238, 7102,  333,  567,  559,  353, 9225,\n         784,   83,  348,   77, 1486,   77,   16,  333, 4428,  347, 1171,  325,\n        1325,  365,   88,  875,  444, 1274,  288, 1350, 7470,  660, 1309, 3409,\n         729,  295,  352,  444, 6709,  601,  286,  457,  303,   18,  631, 1743,\n         327,  291, 4820,  376,  267, 4284, 4826,  290, 4193,   16, 4284, 2721,\n         290, 4193,  295, 2271,  409, 2721,   16, 7418,   31,  444, 2131, 2533,\n         373,  309,  828, 3553, 4864,  295, 1405,  271,   86,  499, 1754, 6139,\n          16,  515, 1141, 1720, 7248, 4725,  353, 7303, 2102, 4849,   18,  631,\n        1791,  353, 1904,  264,  400, 5550, 1057,  603,  335,   16,  295, 4802,\n         384, 2435, 5256,   83,  353, 1904,  264,  400,  315,  363, 1777, 8506,\n        2655,   77,  295,  286,  488,  400,  315, 1045,   69, 4586, 3584,   18,\n         631, 1074,  749,  348,   77, 1486,   77,  409, 3875,   86, 1076,   16,\n         353, 2710,  510,  355,  291,  787,  444, 1342, 6038, 6009,  784,  686,\n         291,  343, 1791,   18,  203,  203, 2025, 1852,  203, 2374, 2278, 4204,\n         203,  384,  540, 1660,  266,   71, 5870,  295,  267, 5578, 4420,  412,\n         512,   30,  836,  809,   17,  706,  809,   30, 2074, 2337,  713,  315,\n        4068, 2999,  290, 4800, 1217,  314, 5870, 2020,  316, 1978,  327, 5578,\n        4420, 1195, 5830,  509,  295,  315,  390, 3716,  813,  203,  203, 1257,\n        5215, 3014,  203,  706, 3589, 3578,  203,  809,  380,   17, 3199, 1446,\n         595,    2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n{'input_ids': tensor([[   0,   48,  351, 4637,  488,  382, 1993, 3317,  277,  469,  384,  382,\n          351, 4637,  488,  382, 1993, 3317,  277,  469, 5450,  352,  262, 2175,\n         4948,  290,  267, 3967,  364,  889,   93,  290,  382,  351, 4637,  469,\n           18,  203,  203, 4482,  225,  203,  432, 4948,  352, 2574,  276,  288,\n         7268,   72, 4064,   18,  203,  203, 2869, 6913,  225,  203,  432, 6397,\n          469,  352,  297, 2615,  288,  262, 7377, 1246,   84,   93, 2364, 8823,\n          353, 5310, 1478,  663,  295, 3865,  511,   88,  694,   18,  203,  203,\n         6742,  383,  361,  203,  203,   38, 9561, 2623,  203,  203,   39,  277,\n          586,  203,  203, 1229,  991,  203, 8589,  383,  364,  889,   93,  290,\n          382,  351, 4637,  469,  203,  203, 9573, 1814, 5061,  295, 1408,  586,\n          634, 9651, 2700,  288, 1419, 9342,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1]])}\n{'input_ids': tensor([   0,   48,  351, 4637,  488,  382, 1993, 3317,  277,  469,  384,  382,\n         351, 4637,  488,  382, 1993, 3317,  277,  469, 5450,  352,  262, 2175,\n        4948,  290,  267, 3967,  364,  889,   93,  290,  382,  351, 4637,  469,\n          18,  203,  203, 4482,  225,  203,  432, 4948,  352, 2574,  276,  288,\n        7268,   72, 4064,   18,  203,  203, 2869, 6913,  225,  203,  432, 6397,\n         469,  352,  297, 2615,  288,  262, 7377, 1246,   84,   93, 2364, 8823,\n         353, 5310, 1478,  663,  295, 3865,  511,   88,  694,   18,  203,  203,\n        6742,  383,  361,  203,  203,   38, 9561, 2623,  203,  203,   39,  277,\n         586,  203,  203, 1229,  991,  203, 8589,  383,  364,  889,   93,  290,\n         382,  351, 4637,  469,  203,  203, 9573, 1814, 5061,  295, 1408,  586,\n         634, 9651, 2700,  288, 1419, 9342,    2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1])}\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"train_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T14:53:53.405928Z","iopub.execute_input":"2025-08-23T14:53:53.406253Z","iopub.status.idle":"2025-08-23T14:53:53.411678Z","shell.execute_reply.started":"2025-08-23T14:53:53.406231Z","shell.execute_reply":"2025-08-23T14:53:53.410953Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask'],\n    num_rows: 5\n})"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# Real text\n\n\nsample_dataset = []\nfor text in train_ds['text']:\n    enc = tokenizer(text, truncation=True, max_length=254, return_tensors=\"pt\")\n    sample_dataset.append({\n        \"input_ids\": enc[\"input_ids\"].squeeze(0),\n        \"attention_mask\": enc[\"attention_mask\"].squeeze(0)\n    })\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T14:33:44.851527Z","iopub.execute_input":"2025-08-23T14:33:44.852239Z","iopub.status.idle":"2025-08-23T14:48:12.250902Z","shell.execute_reply.started":"2025-08-23T14:33:44.852202Z","shell.execute_reply":"2025-08-23T14:48:12.250047Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from transformers import (\n    RobertaConfig,\n    RobertaForMaskedLM,\n    DataCollatorForLanguageModeling,\n    Trainer,\n    TrainingArguments,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:02:57.705800Z","iopub.execute_input":"2025-08-23T15:02:57.706078Z","iopub.status.idle":"2025-08-23T15:03:17.552279Z","shell.execute_reply.started":"2025-08-23T15:02:57.706057Z","shell.execute_reply":"2025-08-23T15:03:17.551502Z"}},"outputs":[{"name":"stderr","text":"2025-08-23 15:03:04.068189: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755961384.251424      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755961384.302565      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"config = RobertaConfig(\nvocab_size = tokenizer.vocab_size,\nhidden_size = 256, #Embedding size\nnum_hidden_layers = 6,#Number of encoder layers\nnum_attention_heads=4, # Attentions per encoder layer, divisible by 256\nintermediate_size = 1024, #FFN layer size\nmax_position_embeddings=256, #Position Embedding max size = embedding size\ntype_vocab_size =1,\nlayer_norm_eps = 1e-5,\ninitializer_range = 0.02,\npad_token_id = 1,\nbos_token_id = 0,\neos_token_id = 2\n)\nmodel = RobertaForMaskedLM(config)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:03:27.321938Z","iopub.execute_input":"2025-08-23T15:03:27.323249Z","iopub.status.idle":"2025-08-23T15:03:27.498994Z","shell.execute_reply.started":"2025-08-23T15:03:27.323216Z","shell.execute_reply":"2025-08-23T15:03:27.498114Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"data_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:03:31.701913Z","iopub.execute_input":"2025-08-23T15:03:31.702605Z","iopub.status.idle":"2025-08-23T15:03:31.707073Z","shell.execute_reply.started":"2025-08-23T15:03:31.702572Z","shell.execute_reply":"2025-08-23T15:03:31.706189Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# 4️⃣ Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./checkpoints\",\n    overwrite_output_dir = False,\n    do_train=True,\n    save_steps=100,\n    logging_steps=100,\n    num_train_epochs=2,\n    per_device_train_batch_size=16,\n    gradient_accumulation_steps=2,\n    warmup_steps = 100,\n    logging_dir = \"./logs\",\n    fp16=True,\n    save_total_limit=3,\n    save_strategy = \"steps\",\n    report_to=[\"tensorboard\"]\n)\n\n# 5️⃣ Create Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=sample_dataset,\n    data_collator=data_collator\n)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:04:29.759616Z","iopub.execute_input":"2025-08-23T15:04:29.759875Z","iopub.status.idle":"2025-08-23T15:24:24.631137Z","shell.execute_reply.started":"2025-08-23T15:04:29.759857Z","shell.execute_reply":"2025-08-23T15:24:24.630536Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5340' max='5340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5340/5340 19:51, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>8.972500</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>8.102400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>7.589400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>7.398800</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>7.347000</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>7.307700</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>7.276300</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>7.240800</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>7.208300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>7.188700</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>7.172400</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>7.153900</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>7.142200</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>7.122500</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>7.101100</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>7.091200</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>7.066300</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>7.055300</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>7.046400</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>7.026000</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>7.022000</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>7.009500</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>6.990900</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>6.985800</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>6.971100</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>6.960400</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>6.926800</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>6.951400</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>6.941300</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>6.947500</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>6.929900</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>6.928700</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>6.917300</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>6.911200</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>6.909000</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>6.908800</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>6.902400</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>6.894000</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>6.894000</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>6.896200</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>6.882200</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>6.892500</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>6.882100</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>6.867700</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>6.869000</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>6.874700</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>6.873300</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>6.867900</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>6.874400</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>6.859800</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>6.881200</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>6.868800</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>6.866400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5340, training_loss=7.068958159600304, metrics={'train_runtime': 1193.4477, 'train_samples_per_second': 286.27, 'train_steps_per_second': 4.474, 'total_flos': 2507229295383552.0, 'train_loss': 7.068958159600304, 'epoch': 2.0})"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"import os\nos.listdir(\"/kaggle/working/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:24:55.685812Z","iopub.execute_input":"2025-08-23T15:24:55.686083Z","iopub.status.idle":"2025-08-23T15:24:55.691365Z","shell.execute_reply.started":"2025-08-23T15:24:55.686062Z","shell.execute_reply":"2025-08-23T15:24:55.690606Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"['.virtual_documents', 'checkpoints', 'merges.txt', 'vocab.json', 'logs']"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"model.save_pretrained(\"./tiny-roberta\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:26:00.390926Z","iopub.execute_input":"2025-08-23T15:26:00.391219Z","iopub.status.idle":"2025-08-23T15:26:00.448381Z","shell.execute_reply.started":"2025-08-23T15:26:00.391196Z","shell.execute_reply":"2025-08-23T15:26:00.447730Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"tokenizer.save_pretrained(\"./tiny-roberta\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:26:17.114812Z","iopub.execute_input":"2025-08-23T15:26:17.115309Z","iopub.status.idle":"2025-08-23T15:26:17.145263Z","shell.execute_reply.started":"2025-08-23T15:26:17.115288Z","shell.execute_reply":"2025-08-23T15:26:17.144653Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"('./tiny-roberta/tokenizer_config.json',\n './tiny-roberta/special_tokens_map.json',\n './tiny-roberta/vocab.json',\n './tiny-roberta/merges.txt',\n './tiny-roberta/added_tokens.json')"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:30:10.503941Z","iopub.execute_input":"2025-08-23T15:30:10.504259Z","iopub.status.idle":"2025-08-23T15:30:10.508115Z","shell.execute_reply.started":"2025-08-23T15:30:10.504237Z","shell.execute_reply":"2025-08-23T15:30:10.507402Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"test_model =  RobertaModel.from_pretrained(\"./tiny-roberta\")\ntest_tokenizer = RobertaTokenizer.from_pretrained(\"./tiny-roberta\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:31:21.927000Z","iopub.execute_input":"2025-08-23T15:31:21.927298Z","iopub.status.idle":"2025-08-23T15:31:21.993200Z","shell.execute_reply.started":"2025-08-23T15:31:21.927277Z","shell.execute_reply":"2025-08-23T15:31:21.992646Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at ./tiny-roberta and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"test_text = \"what are tariffs\"\n\ninputs = test_tokenizer(test_text, return_tensors=\"pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:31:48.663189Z","iopub.execute_input":"2025-08-23T15:31:48.663736Z","iopub.status.idle":"2025-08-23T15:31:48.667569Z","shell.execute_reply.started":"2025-08-23T15:31:48.663712Z","shell.execute_reply":"2025-08-23T15:31:48.666897Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"import torch\nwith torch.no_grad():\n    outputs = test_model(**inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:33:05.507821Z","iopub.execute_input":"2025-08-23T15:33:05.508503Z","iopub.status.idle":"2025-08-23T15:33:05.616460Z","shell.execute_reply.started":"2025-08-23T15:33:05.508479Z","shell.execute_reply":"2025-08-23T15:33:05.615753Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:33:13.215749Z","iopub.execute_input":"2025-08-23T15:33:13.216019Z","iopub.status.idle":"2025-08-23T15:33:13.229693Z","shell.execute_reply.started":"2025-08-23T15:33:13.215998Z","shell.execute_reply":"2025-08-23T15:33:13.228907Z"}},"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.8621,  0.2247,  2.4125,  ...,  0.4460, -0.4612,  0.4734],\n         [-1.1229, -0.6479,  0.5011,  ...,  0.3974,  1.0822,  0.7830],\n         [-1.2034,  0.0680,  1.7291,  ...,  1.4815, -0.0974, -0.8972],\n         ...,\n         [-1.0029,  0.7104,  0.3114,  ..., -0.7080, -0.1807, -0.3309],\n         [-0.3993,  0.8103, -0.4200,  ...,  1.1263,  1.6450, -1.1837],\n         [-1.0440,  1.0778,  1.1312,  ...,  1.5617, -0.4226,  0.1759]]]), pooler_output=tensor([[-3.3787e-01, -6.4649e-01,  3.5569e-01,  3.2207e-01,  1.5847e-01,\n          2.6981e-01,  3.0054e-01,  1.6362e-02,  3.3713e-01,  1.3520e-01,\n          3.5580e-01, -1.2581e-01,  2.2033e-02,  3.9121e-01,  1.1300e-01,\n         -3.8726e-01, -1.8054e-01,  5.3663e-01, -4.2245e-01,  3.3471e-01,\n         -3.4189e-01,  3.9718e-02,  1.4305e-01,  2.5353e-01,  5.8105e-03,\n          5.4468e-01,  2.4620e-01,  3.5678e-01, -1.6053e-01,  1.8096e-01,\n          2.1180e-01, -7.4367e-02,  5.3990e-01, -6.6182e-01,  2.1884e-02,\n         -2.2642e-02, -5.9268e-01, -3.4248e-01, -1.0206e-01, -6.9821e-01,\n          2.4914e-01, -4.2617e-01,  1.7037e-02,  5.3351e-01,  3.1755e-04,\n          1.3168e-01,  8.2996e-02,  1.2922e-01, -2.1078e-02,  3.7556e-01,\n         -1.1955e-01, -3.5340e-01,  4.3110e-01,  5.7821e-02, -4.8686e-01,\n         -1.8148e-01,  5.3535e-01,  1.2047e-01, -1.7349e-01, -5.8775e-02,\n         -5.8018e-04,  1.7775e-01, -2.7908e-01,  2.0385e-01, -3.5945e-01,\n          9.6408e-02,  1.7705e-01, -1.4805e-01, -3.2424e-01,  2.1060e-01,\n          2.7791e-01,  1.9158e-01,  4.0932e-03,  2.4068e-01, -2.5117e-01,\n         -4.9331e-02,  5.8196e-02, -1.3083e-02, -1.7495e-01,  3.2191e-01,\n          2.6379e-01,  1.8441e-01, -2.6938e-01, -3.3738e-01, -6.4075e-02,\n         -4.0201e-02,  4.1607e-01,  1.8651e-01, -5.0147e-01, -2.8504e-01,\n          2.5526e-01,  3.3142e-01,  8.6987e-02,  3.1723e-01,  3.6593e-01,\n          2.3550e-01,  4.6615e-02,  2.4164e-02, -1.7661e-01, -1.7514e-01,\n          3.9335e-02,  6.4127e-01, -1.7423e-01,  2.6592e-01, -2.8370e-01,\n         -3.7983e-01, -2.8105e-01,  4.2875e-01, -4.5329e-01,  5.5942e-01,\n         -8.0068e-01, -2.9300e-01,  2.4888e-01,  2.8189e-01, -3.1250e-01,\n         -8.6204e-02,  1.2133e-01,  3.4470e-01,  7.3801e-02, -3.1672e-01,\n         -9.7913e-02,  2.0272e-01,  1.7653e-01,  8.3866e-02, -3.2681e-01,\n          3.5925e-01,  1.5303e-01, -4.4306e-02, -1.4491e-01,  1.2148e-03,\n          1.6700e-01, -2.0869e-01, -3.2970e-01, -5.3552e-01,  6.9449e-02,\n          6.7453e-01, -1.0722e-01,  9.6590e-02, -1.8802e-01, -3.8181e-01,\n         -2.9368e-02, -3.3906e-01,  4.7112e-01, -2.0675e-01, -2.2176e-02,\n         -3.8742e-01,  1.5168e-01,  4.7507e-01,  1.8966e-02, -1.4114e-01,\n          6.7530e-02, -5.2900e-01, -2.3675e-01, -6.6140e-01,  6.1673e-01,\n          3.2529e-02, -6.0327e-02,  4.6421e-01,  2.4143e-01, -2.1361e-01,\n         -3.1634e-01, -1.6001e-01, -1.9199e-01,  2.4483e-01,  3.1457e-01,\n          1.4172e-01,  1.1506e-01,  3.1127e-02, -3.8725e-01, -2.6531e-01,\n         -6.8807e-02,  4.4741e-01, -2.5618e-01, -7.0506e-03,  3.3952e-01,\n         -2.2504e-01, -1.3037e-01,  3.0059e-01,  1.4822e-01,  3.5866e-01,\n          2.0563e-01, -2.8548e-01, -4.7720e-01,  2.6845e-01,  3.2777e-01,\n          5.9774e-01,  6.6155e-03,  2.3642e-01, -5.4398e-01,  3.8422e-02,\n         -4.9419e-01,  2.5074e-01, -4.3135e-02,  1.5397e-01,  3.0604e-01,\n         -2.5051e-01,  2.4675e-01, -1.1538e-01,  3.6675e-02,  7.5046e-03,\n         -1.4414e-01,  6.0381e-02,  2.3333e-01,  2.6326e-01, -3.9477e-01,\n         -7.8992e-02,  1.3278e-01, -6.3319e-02, -1.2733e-01, -4.5453e-01,\n          3.0052e-01,  2.4424e-01, -3.8931e-01, -3.0239e-02,  1.7146e-01,\n         -4.4637e-02,  2.3605e-01,  3.4785e-01, -5.1960e-01, -2.2387e-01,\n          1.7499e-01,  2.7314e-01, -1.7104e-01, -2.2418e-01, -3.1614e-01,\n         -5.7896e-01, -2.8936e-01,  2.5452e-01,  9.8598e-02, -7.8538e-02,\n         -3.5146e-01,  1.0754e-02,  3.9845e-01, -2.4755e-01, -2.2082e-01,\n         -6.7811e-02, -4.3575e-02, -3.4277e-01,  1.9347e-01, -2.1750e-01,\n         -1.6359e-01,  9.6409e-02,  1.4334e-01,  3.9624e-01, -1.9908e-01,\n          5.1661e-01,  2.9978e-01, -4.3514e-01, -1.4428e-01, -1.6888e-01,\n          9.6614e-02, -3.2256e-02, -3.5426e-01, -7.5314e-03, -3.8863e-01,\n          3.6780e-02]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"last_hidden_state = outputs.last_hidden_state \ncls_embedding = last_hidden_state[:, 0, :]     # [CLS] token representation\nmean_embedding = last_hidden_state.mean(dim=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:34:20.163819Z","iopub.execute_input":"2025-08-23T15:34:20.164530Z","iopub.status.idle":"2025-08-23T15:34:20.168960Z","shell.execute_reply.started":"2025-08-23T15:34:20.164506Z","shell.execute_reply":"2025-08-23T15:34:20.168265Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"cls_embedding.shape, mean_embedding.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:34:39.735412Z","iopub.execute_input":"2025-08-23T15:34:39.736108Z","iopub.status.idle":"2025-08-23T15:34:39.740905Z","shell.execute_reply.started":"2025-08-23T15:34:39.736083Z","shell.execute_reply":"2025-08-23T15:34:39.740042Z"}},"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"(torch.Size([1, 256]), torch.Size([1, 256]))"},"metadata":{}}],"execution_count":58},{"cell_type":"code","source":"expected_output = \"Tariffs are taxes imposed by governments on imported goods and services, used to influence trade, protect domestic industries, and generate revenue.\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:37:15.122603Z","iopub.execute_input":"2025-08-23T15:37:15.123175Z","iopub.status.idle":"2025-08-23T15:37:15.126567Z","shell.execute_reply.started":"2025-08-23T15:37:15.123129Z","shell.execute_reply":"2025-08-23T15:37:15.125888Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"expected_output = test_tokenizer(expected_output, return_tensors=\"pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:37:16.902460Z","iopub.execute_input":"2025-08-23T15:37:16.903323Z","iopub.status.idle":"2025-08-23T15:37:16.907438Z","shell.execute_reply.started":"2025-08-23T15:37:16.903299Z","shell.execute_reply":"2025-08-23T15:37:16.906695Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"import torch\nwith torch.no_grad():\n    outputs_exp = test_model(**expected_output)\n\nlast_hidden_state_exp = outputs_exp.last_hidden_state \ncls_embedding_exp = last_hidden_state_exp[:, 0, :]     # [CLS] token representation\nmean_embedding_exp = last_hidden_state_exp.mean(dim=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:37:18.610716Z","iopub.execute_input":"2025-08-23T15:37:18.611547Z","iopub.status.idle":"2025-08-23T15:37:18.639177Z","shell.execute_reply.started":"2025-08-23T15:37:18.611521Z","shell.execute_reply":"2025-08-23T15:37:18.638559Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"import torch.nn.functional as F\ncos_sim = F.cosine_similarity(mean_embedding, mean_embedding_exp)\nprint(cos_sim.item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:38:27.494785Z","iopub.execute_input":"2025-08-23T15:38:27.495015Z","iopub.status.idle":"2025-08-23T15:38:27.499515Z","shell.execute_reply.started":"2025-08-23T15:38:27.494998Z","shell.execute_reply":"2025-08-23T15:38:27.498805Z"}},"outputs":[{"name":"stdout","text":"0.8750210404396057\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"import json\nimport matplotlib.pyplot as plt\n\n# Path to your trainer_state.json (inside output_dir)\nstate_file = \"./checkpoints/trainer_state.json\"\n\nwith open(state_file, \"r\") as f:\n    trainer_state = json.load(f)\n\n# Extract loss and step history\nsteps = []\nlosses = []\n\nfor log in trainer_state.get(\"log_history\", []):\n    if \"loss\" in log:\n        steps.append(log[\"step\"])\n        losses.append(log[\"loss\"])\n\n# Plot\nplt.figure(figsize=(8, 5))\nplt.plot(steps, losses, marker=\"o\")\nplt.xlabel(\"Step\")\nplt.ylabel(\"Training Loss\")\nplt.title(\"Training Loss over Steps\")\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport matplotlib.pyplot as plt\n\n# Path to your trainer_state.json\nstate_file = \"./checkpoints/trainer_state.json\"\n\nwith open(state_file, \"r\") as f:\n    trainer_state = json.load(f)\n\n# Extract steps, loss, and learning rate\nsteps = []\nlosses = []\nlrs = []\n\nfor log in trainer_state.get(\"log_history\", []):\n    if \"loss\" in log and \"learning_rate\" in log:\n        steps.append(log[\"step\"])\n        losses.append(log[\"loss\"])\n        lrs.append(log[\"learning_rate\"])\n\n# Plot with dual axes\nfig, ax1 = plt.subplots(figsize=(9, 5))\n\ncolor = \"tab:blue\"\nax1.set_xlabel(\"Step\")\nax1.set_ylabel(\"Training Loss\", color=color)\nax1.plot(steps, losses, color=color, marker=\"o\", label=\"Loss\")\nax1.tick_params(axis=\"y\", labelcolor=color)\nax1.grid(True)\n\nax2 = ax1.twinx()  # second y-axis\ncolor = \"tab:red\"\nax2.set_ylabel(\"Learning Rate\", color=color)\nax2.plot(steps, lrs, color=color, linestyle=\"--\", label=\"Learning Rate\")\nax2.tick_params(axis=\"y\", labelcolor=color)\n\nfig.suptitle(\"Training Loss and Learning Rate over Steps\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}