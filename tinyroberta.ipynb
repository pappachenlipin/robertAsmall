{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12830809,"sourceType":"datasetVersion","datasetId":8114587}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-22T14:50:32.144268Z","iopub.execute_input":"2025-08-22T14:50:32.144549Z","iopub.status.idle":"2025-08-22T14:50:32.522618Z","shell.execute_reply.started":"2025-08-22T14:50:32.144527Z","shell.execute_reply":"2025-08-22T14:50:32.521351Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/wiki-cc-openweb/wiki_small_50k/wiki_small_50k.csv\n/kaggle/input/wiki-cc-openweb/openweb_50k (2)/openweb_50k.csv\n/kaggle/input/wiki-cc-openweb/cc_news/cc_news.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from datasets import load_dataset, concatenate_datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T18:28:58.526741Z","iopub.execute_input":"2025-08-23T18:28:58.527079Z","iopub.status.idle":"2025-08-23T18:29:03.408674Z","shell.execute_reply.started":"2025-08-23T18:28:58.527047Z","shell.execute_reply":"2025-08-23T18:29:03.407900Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"wiki_ds = load_dataset(\"csv\", data_files=\"/kaggle/input/wiki-cc-openweb/wiki_small_50k/wiki_small_50k.csv\")\nowt_ds = load_dataset(\"csv\", data_files=\"/kaggle/input/wiki-cc-openweb/openweb_50k (2)/openweb_50k.csv\")\ncc_news_ds = load_dataset(\"csv\", data_files=\"/kaggle/input/wiki-cc-openweb/cc_news/cc_news.csv\")\ntrain_ds = concatenate_datasets([wiki_ds[\"train\"],owt_ds[\"train\"],cc_news_ds[\"train\"]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T18:29:07.305172Z","iopub.execute_input":"2025-08-23T18:29:07.305927Z","iopub.status.idle":"2025-08-23T18:29:23.921147Z","shell.execute_reply.started":"2025-08-23T18:29:07.305901Z","shell.execute_reply":"2025-08-23T18:29:23.920587Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9378aab534c46afb646a2af77201bae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2665bc1b335f485a832cd7f87db75f4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54290e223ac840f5823991c49ca5b28c"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"from tokenizers import ByteLevelBPETokenizer\nspecial_tokens = ['<s>','<pad>','</s>','<unk>','<mask>']\ntokenizer = ByteLevelBPETokenizer()\ntokenizer.train_from_iterator([x['text'] for x in train_ds], vocab_size = 10_000,min_frequency =2, special_tokens=special_tokens)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T18:31:29.685649Z","iopub.execute_input":"2025-08-23T18:31:29.685935Z","iopub.status.idle":"2025-08-23T18:33:25.758959Z","shell.execute_reply.started":"2025-08-23T18:31:29.685910Z","shell.execute_reply":"2025-08-23T18:33:25.758372Z"}},"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"tokenizer.save_model(\"/kaggle/working/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T18:34:07.386461Z","iopub.execute_input":"2025-08-23T18:34:07.386764Z","iopub.status.idle":"2025-08-23T18:34:07.399066Z","shell.execute_reply.started":"2025-08-23T18:34:07.386739Z","shell.execute_reply":"2025-08-23T18:34:07.398207Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/vocab.json', '/kaggle/working/merges.txt']"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from transformers import RobertaTokenizer\ntokenizer = RobertaTokenizer.from_pretrained(\"/kaggle/working/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T18:34:12.910525Z","iopub.execute_input":"2025-08-23T18:34:12.911342Z","iopub.status.idle":"2025-08-23T18:34:12.949138Z","shell.execute_reply.started":"2025-08-23T18:34:12.911304Z","shell.execute_reply":"2025-08-23T18:34:12.948566Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"indices = [0, 2]  # Example indices\nsubset = train_ds.select(indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T14:55:41.894197Z","iopub.execute_input":"2025-08-23T14:55:41.894615Z","iopub.status.idle":"2025-08-23T14:55:41.904805Z","shell.execute_reply.started":"2025-08-23T14:55:41.894580Z","shell.execute_reply":"2025-08-23T14:55:41.903913Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"type(subset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T14:55:43.801822Z","iopub.execute_input":"2025-08-23T14:55:43.802446Z","iopub.status.idle":"2025-08-23T14:55:43.807308Z","shell.execute_reply.started":"2025-08-23T14:55:43.802420Z","shell.execute_reply":"2025-08-23T14:55:43.806525Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"datasets.arrow_dataset.Dataset"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"def tokenize(batch):\n    result = tokenizer(batch[\"text\"], truncation=True, max_length=254,return_tensors=\"pt\" )\n    print(result)\n    result = {\"input_ids\": result[\"input_ids\"].squeeze(0),\n    \"attention_mask\":result[\"attention_mask\"].squeeze(0)}\n    print(result)\n    return result\n\ntrain_dataset = subset.map(tokenize, remove_columns=[\"text\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:02:18.283107Z","iopub.execute_input":"2025-08-23T15:02:18.283400Z","iopub.status.idle":"2025-08-23T15:02:18.483442Z","shell.execute_reply.started":"2025-08-23T15:02:18.283379Z","shell.execute_reply":"2025-08-23T15:02:18.482668Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a61457829fe406f9764e43828261379"}},"metadata":{}},{"name":"stdout","text":"{'input_ids': tensor([[   0, 9634, 2278, 4204, 2374, 7303, 2278, 4204,  365,  809, 1270,  836,\n         5215,  688, 1275, 1939, 2763,   25,   13,  352,  368, 2743, 2875,  400,\n           18,  203,  203, 7994,  203, 1238, 7102,  333,  567,  559,  353, 9225,\n          784,   83,  348,   77, 1486,   77,   16,  333, 4428,  347, 1171,  325,\n         1325,  365,   88,  875,  444, 1274,  288, 1350, 7470,  660, 1309, 3409,\n          729,  295,  352,  444, 6709,  601,  286,  457,  303,   18,  631, 1743,\n          327,  291, 4820,  376,  267, 4284, 4826,  290, 4193,   16, 4284, 2721,\n          290, 4193,  295, 2271,  409, 2721,   16, 7418,   31,  444, 2131, 2533,\n          373,  309,  828, 3553, 4864,  295, 1405,  271,   86,  499, 1754, 6139,\n           16,  515, 1141, 1720, 7248, 4725,  353, 7303, 2102, 4849,   18,  631,\n         1791,  353, 1904,  264,  400, 5550, 1057,  603,  335,   16,  295, 4802,\n          384, 2435, 5256,   83,  353, 1904,  264,  400,  315,  363, 1777, 8506,\n         2655,   77,  295,  286,  488,  400,  315, 1045,   69, 4586, 3584,   18,\n          631, 1074,  749,  348,   77, 1486,   77,  409, 3875,   86, 1076,   16,\n          353, 2710,  510,  355,  291,  787,  444, 1342, 6038, 6009,  784,  686,\n          291,  343, 1791,   18,  203,  203, 2025, 1852,  203, 2374, 2278, 4204,\n          203,  384,  540, 1660,  266,   71, 5870,  295,  267, 5578, 4420,  412,\n          512,   30,  836,  809,   17,  706,  809,   30, 2074, 2337,  713,  315,\n         4068, 2999,  290, 4800, 1217,  314, 5870, 2020,  316, 1978,  327, 5578,\n         4420, 1195, 5830,  509,  295,  315,  390, 3716,  813,  203,  203, 1257,\n         5215, 3014,  203,  706, 3589, 3578,  203,  809,  380,   17, 3199, 1446,\n          595,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n{'input_ids': tensor([   0, 9634, 2278, 4204, 2374, 7303, 2278, 4204,  365,  809, 1270,  836,\n        5215,  688, 1275, 1939, 2763,   25,   13,  352,  368, 2743, 2875,  400,\n          18,  203,  203, 7994,  203, 1238, 7102,  333,  567,  559,  353, 9225,\n         784,   83,  348,   77, 1486,   77,   16,  333, 4428,  347, 1171,  325,\n        1325,  365,   88,  875,  444, 1274,  288, 1350, 7470,  660, 1309, 3409,\n         729,  295,  352,  444, 6709,  601,  286,  457,  303,   18,  631, 1743,\n         327,  291, 4820,  376,  267, 4284, 4826,  290, 4193,   16, 4284, 2721,\n         290, 4193,  295, 2271,  409, 2721,   16, 7418,   31,  444, 2131, 2533,\n         373,  309,  828, 3553, 4864,  295, 1405,  271,   86,  499, 1754, 6139,\n          16,  515, 1141, 1720, 7248, 4725,  353, 7303, 2102, 4849,   18,  631,\n        1791,  353, 1904,  264,  400, 5550, 1057,  603,  335,   16,  295, 4802,\n         384, 2435, 5256,   83,  353, 1904,  264,  400,  315,  363, 1777, 8506,\n        2655,   77,  295,  286,  488,  400,  315, 1045,   69, 4586, 3584,   18,\n         631, 1074,  749,  348,   77, 1486,   77,  409, 3875,   86, 1076,   16,\n         353, 2710,  510,  355,  291,  787,  444, 1342, 6038, 6009,  784,  686,\n         291,  343, 1791,   18,  203,  203, 2025, 1852,  203, 2374, 2278, 4204,\n         203,  384,  540, 1660,  266,   71, 5870,  295,  267, 5578, 4420,  412,\n         512,   30,  836,  809,   17,  706,  809,   30, 2074, 2337,  713,  315,\n        4068, 2999,  290, 4800, 1217,  314, 5870, 2020,  316, 1978,  327, 5578,\n        4420, 1195, 5830,  509,  295,  315,  390, 3716,  813,  203,  203, 1257,\n        5215, 3014,  203,  706, 3589, 3578,  203,  809,  380,   17, 3199, 1446,\n         595,    2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n{'input_ids': tensor([[   0,   48,  351, 4637,  488,  382, 1993, 3317,  277,  469,  384,  382,\n          351, 4637,  488,  382, 1993, 3317,  277,  469, 5450,  352,  262, 2175,\n         4948,  290,  267, 3967,  364,  889,   93,  290,  382,  351, 4637,  469,\n           18,  203,  203, 4482,  225,  203,  432, 4948,  352, 2574,  276,  288,\n         7268,   72, 4064,   18,  203,  203, 2869, 6913,  225,  203,  432, 6397,\n          469,  352,  297, 2615,  288,  262, 7377, 1246,   84,   93, 2364, 8823,\n          353, 5310, 1478,  663,  295, 3865,  511,   88,  694,   18,  203,  203,\n         6742,  383,  361,  203,  203,   38, 9561, 2623,  203,  203,   39,  277,\n          586,  203,  203, 1229,  991,  203, 8589,  383,  364,  889,   93,  290,\n          382,  351, 4637,  469,  203,  203, 9573, 1814, 5061,  295, 1408,  586,\n          634, 9651, 2700,  288, 1419, 9342,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1]])}\n{'input_ids': tensor([   0,   48,  351, 4637,  488,  382, 1993, 3317,  277,  469,  384,  382,\n         351, 4637,  488,  382, 1993, 3317,  277,  469, 5450,  352,  262, 2175,\n        4948,  290,  267, 3967,  364,  889,   93,  290,  382,  351, 4637,  469,\n          18,  203,  203, 4482,  225,  203,  432, 4948,  352, 2574,  276,  288,\n        7268,   72, 4064,   18,  203,  203, 2869, 6913,  225,  203,  432, 6397,\n         469,  352,  297, 2615,  288,  262, 7377, 1246,   84,   93, 2364, 8823,\n         353, 5310, 1478,  663,  295, 3865,  511,   88,  694,   18,  203,  203,\n        6742,  383,  361,  203,  203,   38, 9561, 2623,  203,  203,   39,  277,\n         586,  203,  203, 1229,  991,  203, 8589,  383,  364,  889,   93,  290,\n         382,  351, 4637,  469,  203,  203, 9573, 1814, 5061,  295, 1408,  586,\n         634, 9651, 2700,  288, 1419, 9342,    2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1])}\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"train_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T14:53:53.405928Z","iopub.execute_input":"2025-08-23T14:53:53.406253Z","iopub.status.idle":"2025-08-23T14:53:53.411678Z","shell.execute_reply.started":"2025-08-23T14:53:53.406231Z","shell.execute_reply":"2025-08-23T14:53:53.410953Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask'],\n    num_rows: 5\n})"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# Real text\n\n\nsample_dataset = []\nfor text in train_ds['text']:\n    enc = tokenizer(text, truncation=True, max_length=254, return_tensors=\"pt\")\n    sample_dataset.append({\n        \"input_ids\": enc[\"input_ids\"].squeeze(0),\n        \"attention_mask\": enc[\"attention_mask\"].squeeze(0)\n    })\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T18:34:24.790773Z","iopub.execute_input":"2025-08-23T18:34:24.791756Z","iopub.status.idle":"2025-08-23T18:48:30.640017Z","shell.execute_reply.started":"2025-08-23T18:34:24.791724Z","shell.execute_reply":"2025-08-23T18:48:30.639455Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from transformers import (\n    RobertaConfig,\n    RobertaForMaskedLM,\n\n    DataCollatorForLanguageModeling,\n    Trainer,\n    TrainingArguments,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T18:49:24.486405Z","iopub.execute_input":"2025-08-23T18:49:24.486672Z","iopub.status.idle":"2025-08-23T18:49:55.128290Z","shell.execute_reply.started":"2025-08-23T18:49:24.486648Z","shell.execute_reply":"2025-08-23T18:49:55.127509Z"}},"outputs":[{"name":"stderr","text":"2025-08-23 18:49:34.881904: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755974975.254754      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755974975.353849      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"config = RobertaConfig(\nvocab_size = tokenizer.vocab_size,\nhidden_size = 256, #Embedding size\nnum_hidden_layers = 6,#Number of encoder layers\nnum_attention_heads=4, # Attentions per encoder layer, divisible by 256\nintermediate_size = 1024, #FFN layer size\nmax_position_embeddings=256, #Position Embedding max size = embedding size\ntype_vocab_size =1,\nlayer_norm_eps = 1e-5,\ninitializer_range = 0.02,\npad_token_id = 1,\nbos_token_id = 0,\neos_token_id = 2\n)\nmodel = RobertaForMaskedLM(config)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T18:49:55.129786Z","iopub.execute_input":"2025-08-23T18:49:55.130330Z","iopub.status.idle":"2025-08-23T18:49:55.326804Z","shell.execute_reply.started":"2025-08-23T18:49:55.130309Z","shell.execute_reply":"2025-08-23T18:49:55.325923Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"data_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T18:50:15.743667Z","iopub.execute_input":"2025-08-23T18:50:15.744315Z","iopub.status.idle":"2025-08-23T18:50:15.748124Z","shell.execute_reply.started":"2025-08-23T18:50:15.744291Z","shell.execute_reply":"2025-08-23T18:50:15.747453Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# 4️⃣ Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./checkpoints\",\n    overwrite_output_dir = False,\n    do_train=True,\n    save_steps=100,\n    logging_steps=100,\n    num_train_epochs=15,\n    per_device_train_batch_size=16,\n    gradient_accumulation_steps=2,\n    warmup_steps = 100,\n    logging_dir = \"./logs\",\n    fp16=True,\n    save_total_limit=3,\n    save_strategy = \"steps\",\n    report_to=[\"tensorboard\"]\n)\n\n# 5️⃣ Create Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=sample_dataset,\n    data_collator=data_collator\n)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T18:50:29.787687Z","iopub.execute_input":"2025-08-23T18:50:29.788479Z","execution_failed":"2025-08-23T23:41:09.876Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='36061' max='40050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [36061/40050 2:16:29 < 15:06, 4.40 it/s, Epoch 13.51/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>8.935700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>8.087100</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>7.571400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>7.391100</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>7.339800</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>7.298800</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>7.265300</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>7.228900</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>7.193200</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>7.168100</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>7.148200</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>7.128100</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>7.114000</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>7.090700</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>7.069000</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>7.055400</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>7.027000</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>7.013500</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>7.002500</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>6.978700</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>6.972500</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>6.957500</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>6.938000</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>6.930100</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>6.913200</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>6.900800</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>6.864800</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>6.885600</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>6.873100</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>6.878600</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>6.856300</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>6.853700</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>6.837900</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>6.827900</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>6.825500</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>6.819900</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>6.810900</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>6.797000</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>6.793800</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>6.792300</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>6.774400</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>6.781600</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>6.766500</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>6.747900</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>6.744400</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>6.744100</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>6.739700</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>6.727900</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>6.728500</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>6.710500</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>6.727000</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>6.708000</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>6.699700</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>6.657200</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>6.690000</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>6.694800</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>6.686400</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>6.671700</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>6.662200</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>6.652000</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>6.662900</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>6.649800</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>6.651600</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>6.638100</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>6.636100</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>6.637800</td>\n    </tr>\n    <tr>\n      <td>6700</td>\n      <td>6.616500</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>6.625900</td>\n    </tr>\n    <tr>\n      <td>6900</td>\n      <td>6.609800</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>6.608600</td>\n    </tr>\n    <tr>\n      <td>7100</td>\n      <td>6.598900</td>\n    </tr>\n    <tr>\n      <td>7200</td>\n      <td>6.598100</td>\n    </tr>\n    <tr>\n      <td>7300</td>\n      <td>6.592900</td>\n    </tr>\n    <tr>\n      <td>7400</td>\n      <td>6.587700</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>6.590800</td>\n    </tr>\n    <tr>\n      <td>7600</td>\n      <td>6.580100</td>\n    </tr>\n    <tr>\n      <td>7700</td>\n      <td>6.574500</td>\n    </tr>\n    <tr>\n      <td>7800</td>\n      <td>6.572300</td>\n    </tr>\n    <tr>\n      <td>7900</td>\n      <td>6.569700</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>6.554400</td>\n    </tr>\n    <tr>\n      <td>8100</td>\n      <td>6.519200</td>\n    </tr>\n    <tr>\n      <td>8200</td>\n      <td>6.551100</td>\n    </tr>\n    <tr>\n      <td>8300</td>\n      <td>6.545500</td>\n    </tr>\n    <tr>\n      <td>8400</td>\n      <td>6.550900</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>6.535800</td>\n    </tr>\n    <tr>\n      <td>8600</td>\n      <td>6.534400</td>\n    </tr>\n    <tr>\n      <td>8700</td>\n      <td>6.548900</td>\n    </tr>\n    <tr>\n      <td>8800</td>\n      <td>6.524800</td>\n    </tr>\n    <tr>\n      <td>8900</td>\n      <td>6.538500</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>6.534700</td>\n    </tr>\n    <tr>\n      <td>9100</td>\n      <td>6.525800</td>\n    </tr>\n    <tr>\n      <td>9200</td>\n      <td>6.521300</td>\n    </tr>\n    <tr>\n      <td>9300</td>\n      <td>6.513500</td>\n    </tr>\n    <tr>\n      <td>9400</td>\n      <td>6.509700</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>6.491800</td>\n    </tr>\n    <tr>\n      <td>9600</td>\n      <td>6.506600</td>\n    </tr>\n    <tr>\n      <td>9700</td>\n      <td>6.506700</td>\n    </tr>\n    <tr>\n      <td>9800</td>\n      <td>6.484400</td>\n    </tr>\n    <tr>\n      <td>9900</td>\n      <td>6.487600</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>6.495700</td>\n    </tr>\n    <tr>\n      <td>10100</td>\n      <td>6.456500</td>\n    </tr>\n    <tr>\n      <td>10200</td>\n      <td>6.477700</td>\n    </tr>\n    <tr>\n      <td>10300</td>\n      <td>6.476900</td>\n    </tr>\n    <tr>\n      <td>10400</td>\n      <td>6.465000</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>6.477500</td>\n    </tr>\n    <tr>\n      <td>10600</td>\n      <td>6.461200</td>\n    </tr>\n    <tr>\n      <td>10700</td>\n      <td>6.436100</td>\n    </tr>\n    <tr>\n      <td>10800</td>\n      <td>6.455300</td>\n    </tr>\n    <tr>\n      <td>10900</td>\n      <td>6.455600</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>6.457000</td>\n    </tr>\n    <tr>\n      <td>11100</td>\n      <td>6.448900</td>\n    </tr>\n    <tr>\n      <td>11200</td>\n      <td>6.444200</td>\n    </tr>\n    <tr>\n      <td>11300</td>\n      <td>6.431000</td>\n    </tr>\n    <tr>\n      <td>11400</td>\n      <td>6.446500</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>6.432300</td>\n    </tr>\n    <tr>\n      <td>11600</td>\n      <td>6.438000</td>\n    </tr>\n    <tr>\n      <td>11700</td>\n      <td>6.425600</td>\n    </tr>\n    <tr>\n      <td>11800</td>\n      <td>6.424800</td>\n    </tr>\n    <tr>\n      <td>11900</td>\n      <td>6.426900</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>6.424800</td>\n    </tr>\n    <tr>\n      <td>12100</td>\n      <td>6.423200</td>\n    </tr>\n    <tr>\n      <td>12200</td>\n      <td>6.421100</td>\n    </tr>\n    <tr>\n      <td>12300</td>\n      <td>6.420300</td>\n    </tr>\n    <tr>\n      <td>12400</td>\n      <td>6.424500</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>6.406500</td>\n    </tr>\n    <tr>\n      <td>12600</td>\n      <td>6.407800</td>\n    </tr>\n    <tr>\n      <td>12700</td>\n      <td>6.403000</td>\n    </tr>\n    <tr>\n      <td>12800</td>\n      <td>6.395300</td>\n    </tr>\n    <tr>\n      <td>12900</td>\n      <td>6.400200</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>6.398400</td>\n    </tr>\n    <tr>\n      <td>13100</td>\n      <td>6.396000</td>\n    </tr>\n    <tr>\n      <td>13200</td>\n      <td>6.392900</td>\n    </tr>\n    <tr>\n      <td>13300</td>\n      <td>6.393800</td>\n    </tr>\n    <tr>\n      <td>13400</td>\n      <td>6.362300</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>6.380100</td>\n    </tr>\n    <tr>\n      <td>13600</td>\n      <td>6.378600</td>\n    </tr>\n    <tr>\n      <td>13700</td>\n      <td>6.356500</td>\n    </tr>\n    <tr>\n      <td>13800</td>\n      <td>6.375700</td>\n    </tr>\n    <tr>\n      <td>13900</td>\n      <td>6.372900</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>6.372700</td>\n    </tr>\n    <tr>\n      <td>14100</td>\n      <td>6.357100</td>\n    </tr>\n    <tr>\n      <td>14200</td>\n      <td>6.361200</td>\n    </tr>\n    <tr>\n      <td>14300</td>\n      <td>6.357100</td>\n    </tr>\n    <tr>\n      <td>14400</td>\n      <td>6.354200</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>6.355000</td>\n    </tr>\n    <tr>\n      <td>14600</td>\n      <td>6.357000</td>\n    </tr>\n    <tr>\n      <td>14700</td>\n      <td>6.351500</td>\n    </tr>\n    <tr>\n      <td>14800</td>\n      <td>6.342500</td>\n    </tr>\n    <tr>\n      <td>14900</td>\n      <td>6.346300</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>6.345300</td>\n    </tr>\n    <tr>\n      <td>15100</td>\n      <td>6.347100</td>\n    </tr>\n    <tr>\n      <td>15200</td>\n      <td>6.330400</td>\n    </tr>\n    <tr>\n      <td>15300</td>\n      <td>6.333400</td>\n    </tr>\n    <tr>\n      <td>15400</td>\n      <td>6.334800</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>6.336000</td>\n    </tr>\n    <tr>\n      <td>15600</td>\n      <td>6.328800</td>\n    </tr>\n    <tr>\n      <td>15700</td>\n      <td>6.328400</td>\n    </tr>\n    <tr>\n      <td>15800</td>\n      <td>6.333600</td>\n    </tr>\n    <tr>\n      <td>15900</td>\n      <td>6.331200</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>6.317500</td>\n    </tr>\n    <tr>\n      <td>16100</td>\n      <td>6.283800</td>\n    </tr>\n    <tr>\n      <td>16200</td>\n      <td>6.304900</td>\n    </tr>\n    <tr>\n      <td>16300</td>\n      <td>6.301000</td>\n    </tr>\n    <tr>\n      <td>16400</td>\n      <td>6.311300</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>6.296700</td>\n    </tr>\n    <tr>\n      <td>16600</td>\n      <td>6.290600</td>\n    </tr>\n    <tr>\n      <td>16700</td>\n      <td>6.309700</td>\n    </tr>\n    <tr>\n      <td>16800</td>\n      <td>6.286600</td>\n    </tr>\n    <tr>\n      <td>16900</td>\n      <td>6.294700</td>\n    </tr>\n    <tr>\n      <td>17000</td>\n      <td>6.300400</td>\n    </tr>\n    <tr>\n      <td>17100</td>\n      <td>6.295200</td>\n    </tr>\n    <tr>\n      <td>17200</td>\n      <td>6.294500</td>\n    </tr>\n    <tr>\n      <td>17300</td>\n      <td>6.289500</td>\n    </tr>\n    <tr>\n      <td>17400</td>\n      <td>6.278600</td>\n    </tr>\n    <tr>\n      <td>17500</td>\n      <td>6.278800</td>\n    </tr>\n    <tr>\n      <td>17600</td>\n      <td>6.267500</td>\n    </tr>\n    <tr>\n      <td>17700</td>\n      <td>6.266600</td>\n    </tr>\n    <tr>\n      <td>17800</td>\n      <td>6.271700</td>\n    </tr>\n    <tr>\n      <td>17900</td>\n      <td>6.258800</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>6.268200</td>\n    </tr>\n    <tr>\n      <td>18100</td>\n      <td>6.269400</td>\n    </tr>\n    <tr>\n      <td>18200</td>\n      <td>6.261900</td>\n    </tr>\n    <tr>\n      <td>18300</td>\n      <td>6.271000</td>\n    </tr>\n    <tr>\n      <td>18400</td>\n      <td>6.257800</td>\n    </tr>\n    <tr>\n      <td>18500</td>\n      <td>6.251500</td>\n    </tr>\n    <tr>\n      <td>18600</td>\n      <td>6.253800</td>\n    </tr>\n    <tr>\n      <td>18700</td>\n      <td>6.220600</td>\n    </tr>\n    <tr>\n      <td>18800</td>\n      <td>6.246900</td>\n    </tr>\n    <tr>\n      <td>18900</td>\n      <td>6.244800</td>\n    </tr>\n    <tr>\n      <td>19000</td>\n      <td>6.235900</td>\n    </tr>\n    <tr>\n      <td>19100</td>\n      <td>6.242000</td>\n    </tr>\n    <tr>\n      <td>19200</td>\n      <td>6.242500</td>\n    </tr>\n    <tr>\n      <td>19300</td>\n      <td>6.239500</td>\n    </tr>\n    <tr>\n      <td>19400</td>\n      <td>6.224900</td>\n    </tr>\n    <tr>\n      <td>19500</td>\n      <td>6.224000</td>\n    </tr>\n    <tr>\n      <td>19600</td>\n      <td>6.229100</td>\n    </tr>\n    <tr>\n      <td>19700</td>\n      <td>6.209700</td>\n    </tr>\n    <tr>\n      <td>19800</td>\n      <td>6.221100</td>\n    </tr>\n    <tr>\n      <td>19900</td>\n      <td>6.221400</td>\n    </tr>\n    <tr>\n      <td>20000</td>\n      <td>6.213400</td>\n    </tr>\n    <tr>\n      <td>20100</td>\n      <td>6.212200</td>\n    </tr>\n    <tr>\n      <td>20200</td>\n      <td>6.210900</td>\n    </tr>\n    <tr>\n      <td>20300</td>\n      <td>6.207300</td>\n    </tr>\n    <tr>\n      <td>20400</td>\n      <td>6.201200</td>\n    </tr>\n    <tr>\n      <td>20500</td>\n      <td>6.197600</td>\n    </tr>\n    <tr>\n      <td>20600</td>\n      <td>6.199200</td>\n    </tr>\n    <tr>\n      <td>20700</td>\n      <td>6.188100</td>\n    </tr>\n    <tr>\n      <td>20800</td>\n      <td>6.198500</td>\n    </tr>\n    <tr>\n      <td>20900</td>\n      <td>6.192100</td>\n    </tr>\n    <tr>\n      <td>21000</td>\n      <td>6.189100</td>\n    </tr>\n    <tr>\n      <td>21100</td>\n      <td>6.190300</td>\n    </tr>\n    <tr>\n      <td>21200</td>\n      <td>6.195800</td>\n    </tr>\n    <tr>\n      <td>21300</td>\n      <td>6.172800</td>\n    </tr>\n    <tr>\n      <td>21400</td>\n      <td>6.148500</td>\n    </tr>\n    <tr>\n      <td>21500</td>\n      <td>6.172300</td>\n    </tr>\n    <tr>\n      <td>21600</td>\n      <td>6.182800</td>\n    </tr>\n    <tr>\n      <td>21700</td>\n      <td>6.178200</td>\n    </tr>\n    <tr>\n      <td>21800</td>\n      <td>6.179400</td>\n    </tr>\n    <tr>\n      <td>21900</td>\n      <td>6.177700</td>\n    </tr>\n    <tr>\n      <td>22000</td>\n      <td>6.166500</td>\n    </tr>\n    <tr>\n      <td>22100</td>\n      <td>6.160800</td>\n    </tr>\n    <tr>\n      <td>22200</td>\n      <td>6.179300</td>\n    </tr>\n    <tr>\n      <td>22300</td>\n      <td>6.165200</td>\n    </tr>\n    <tr>\n      <td>22400</td>\n      <td>6.161000</td>\n    </tr>\n    <tr>\n      <td>22500</td>\n      <td>6.146000</td>\n    </tr>\n    <tr>\n      <td>22600</td>\n      <td>6.158300</td>\n    </tr>\n    <tr>\n      <td>22700</td>\n      <td>6.150500</td>\n    </tr>\n    <tr>\n      <td>22800</td>\n      <td>6.154900</td>\n    </tr>\n    <tr>\n      <td>22900</td>\n      <td>6.147300</td>\n    </tr>\n    <tr>\n      <td>23000</td>\n      <td>6.140700</td>\n    </tr>\n    <tr>\n      <td>23100</td>\n      <td>6.154600</td>\n    </tr>\n    <tr>\n      <td>23200</td>\n      <td>6.139300</td>\n    </tr>\n    <tr>\n      <td>23300</td>\n      <td>6.134200</td>\n    </tr>\n    <tr>\n      <td>23400</td>\n      <td>6.141100</td>\n    </tr>\n    <tr>\n      <td>23500</td>\n      <td>6.135100</td>\n    </tr>\n    <tr>\n      <td>23600</td>\n      <td>6.133200</td>\n    </tr>\n    <tr>\n      <td>23700</td>\n      <td>6.132900</td>\n    </tr>\n    <tr>\n      <td>23800</td>\n      <td>6.126300</td>\n    </tr>\n    <tr>\n      <td>23900</td>\n      <td>6.118000</td>\n    </tr>\n    <tr>\n      <td>24000</td>\n      <td>6.124000</td>\n    </tr>\n    <tr>\n      <td>24100</td>\n      <td>6.091900</td>\n    </tr>\n    <tr>\n      <td>24200</td>\n      <td>6.120200</td>\n    </tr>\n    <tr>\n      <td>24300</td>\n      <td>6.114500</td>\n    </tr>\n    <tr>\n      <td>24400</td>\n      <td>6.116700</td>\n    </tr>\n    <tr>\n      <td>24500</td>\n      <td>6.111900</td>\n    </tr>\n    <tr>\n      <td>24600</td>\n      <td>6.116700</td>\n    </tr>\n    <tr>\n      <td>24700</td>\n      <td>6.119800</td>\n    </tr>\n    <tr>\n      <td>24800</td>\n      <td>6.100800</td>\n    </tr>\n    <tr>\n      <td>24900</td>\n      <td>6.123100</td>\n    </tr>\n    <tr>\n      <td>25000</td>\n      <td>6.109300</td>\n    </tr>\n    <tr>\n      <td>25100</td>\n      <td>6.097100</td>\n    </tr>\n    <tr>\n      <td>25200</td>\n      <td>6.106800</td>\n    </tr>\n    <tr>\n      <td>25300</td>\n      <td>6.121300</td>\n    </tr>\n    <tr>\n      <td>25400</td>\n      <td>6.093600</td>\n    </tr>\n    <tr>\n      <td>25500</td>\n      <td>6.095000</td>\n    </tr>\n    <tr>\n      <td>25600</td>\n      <td>6.093700</td>\n    </tr>\n    <tr>\n      <td>25700</td>\n      <td>6.104000</td>\n    </tr>\n    <tr>\n      <td>25800</td>\n      <td>6.094600</td>\n    </tr>\n    <tr>\n      <td>25900</td>\n      <td>6.081800</td>\n    </tr>\n    <tr>\n      <td>26000</td>\n      <td>6.088100</td>\n    </tr>\n    <tr>\n      <td>26100</td>\n      <td>6.079100</td>\n    </tr>\n    <tr>\n      <td>26200</td>\n      <td>6.088600</td>\n    </tr>\n    <tr>\n      <td>26300</td>\n      <td>6.075300</td>\n    </tr>\n    <tr>\n      <td>26400</td>\n      <td>6.080300</td>\n    </tr>\n    <tr>\n      <td>26500</td>\n      <td>6.081800</td>\n    </tr>\n    <tr>\n      <td>26600</td>\n      <td>6.086700</td>\n    </tr>\n    <tr>\n      <td>26700</td>\n      <td>6.053400</td>\n    </tr>\n    <tr>\n      <td>26800</td>\n      <td>6.073400</td>\n    </tr>\n    <tr>\n      <td>26900</td>\n      <td>6.062600</td>\n    </tr>\n    <tr>\n      <td>27000</td>\n      <td>6.063900</td>\n    </tr>\n    <tr>\n      <td>27100</td>\n      <td>6.059100</td>\n    </tr>\n    <tr>\n      <td>27200</td>\n      <td>6.068400</td>\n    </tr>\n    <tr>\n      <td>27300</td>\n      <td>6.066100</td>\n    </tr>\n    <tr>\n      <td>27400</td>\n      <td>6.070900</td>\n    </tr>\n    <tr>\n      <td>27500</td>\n      <td>6.075500</td>\n    </tr>\n    <tr>\n      <td>27600</td>\n      <td>6.061800</td>\n    </tr>\n    <tr>\n      <td>27700</td>\n      <td>6.060800</td>\n    </tr>\n    <tr>\n      <td>27800</td>\n      <td>6.053300</td>\n    </tr>\n    <tr>\n      <td>27900</td>\n      <td>6.049200</td>\n    </tr>\n    <tr>\n      <td>28000</td>\n      <td>6.063100</td>\n    </tr>\n    <tr>\n      <td>28100</td>\n      <td>6.055900</td>\n    </tr>\n    <tr>\n      <td>28200</td>\n      <td>6.043800</td>\n    </tr>\n    <tr>\n      <td>28300</td>\n      <td>6.056700</td>\n    </tr>\n    <tr>\n      <td>28400</td>\n      <td>6.052600</td>\n    </tr>\n    <tr>\n      <td>28500</td>\n      <td>6.056600</td>\n    </tr>\n    <tr>\n      <td>28600</td>\n      <td>6.051100</td>\n    </tr>\n    <tr>\n      <td>28700</td>\n      <td>6.043100</td>\n    </tr>\n    <tr>\n      <td>28800</td>\n      <td>6.048300</td>\n    </tr>\n    <tr>\n      <td>28900</td>\n      <td>6.049900</td>\n    </tr>\n    <tr>\n      <td>29000</td>\n      <td>6.040600</td>\n    </tr>\n    <tr>\n      <td>29100</td>\n      <td>6.039500</td>\n    </tr>\n    <tr>\n      <td>29200</td>\n      <td>6.042600</td>\n    </tr>\n    <tr>\n      <td>29300</td>\n      <td>6.035700</td>\n    </tr>\n    <tr>\n      <td>29400</td>\n      <td>6.005200</td>\n    </tr>\n    <tr>\n      <td>29500</td>\n      <td>6.042700</td>\n    </tr>\n    <tr>\n      <td>29600</td>\n      <td>6.028100</td>\n    </tr>\n    <tr>\n      <td>29700</td>\n      <td>6.027900</td>\n    </tr>\n    <tr>\n      <td>29800</td>\n      <td>6.034000</td>\n    </tr>\n    <tr>\n      <td>29900</td>\n      <td>6.024300</td>\n    </tr>\n    <tr>\n      <td>30000</td>\n      <td>6.031200</td>\n    </tr>\n    <tr>\n      <td>30100</td>\n      <td>6.024400</td>\n    </tr>\n    <tr>\n      <td>30200</td>\n      <td>6.032800</td>\n    </tr>\n    <tr>\n      <td>30300</td>\n      <td>6.023200</td>\n    </tr>\n    <tr>\n      <td>30400</td>\n      <td>6.016000</td>\n    </tr>\n    <tr>\n      <td>30500</td>\n      <td>6.025900</td>\n    </tr>\n    <tr>\n      <td>30600</td>\n      <td>6.026300</td>\n    </tr>\n    <tr>\n      <td>30700</td>\n      <td>6.028900</td>\n    </tr>\n    <tr>\n      <td>30800</td>\n      <td>6.008200</td>\n    </tr>\n    <tr>\n      <td>30900</td>\n      <td>6.010300</td>\n    </tr>\n    <tr>\n      <td>31000</td>\n      <td>6.020900</td>\n    </tr>\n    <tr>\n      <td>31100</td>\n      <td>6.009100</td>\n    </tr>\n    <tr>\n      <td>31200</td>\n      <td>6.033800</td>\n    </tr>\n    <tr>\n      <td>31300</td>\n      <td>6.009200</td>\n    </tr>\n    <tr>\n      <td>31400</td>\n      <td>6.017700</td>\n    </tr>\n    <tr>\n      <td>31500</td>\n      <td>6.013400</td>\n    </tr>\n    <tr>\n      <td>31600</td>\n      <td>6.002400</td>\n    </tr>\n    <tr>\n      <td>31700</td>\n      <td>6.019800</td>\n    </tr>\n    <tr>\n      <td>31800</td>\n      <td>6.013200</td>\n    </tr>\n    <tr>\n      <td>31900</td>\n      <td>6.012800</td>\n    </tr>\n    <tr>\n      <td>32000</td>\n      <td>6.002700</td>\n    </tr>\n    <tr>\n      <td>32100</td>\n      <td>5.967500</td>\n    </tr>\n    <tr>\n      <td>32200</td>\n      <td>6.002600</td>\n    </tr>\n    <tr>\n      <td>32300</td>\n      <td>6.004400</td>\n    </tr>\n    <tr>\n      <td>32400</td>\n      <td>6.003400</td>\n    </tr>\n    <tr>\n      <td>32500</td>\n      <td>5.999300</td>\n    </tr>\n    <tr>\n      <td>32600</td>\n      <td>5.994800</td>\n    </tr>\n    <tr>\n      <td>32700</td>\n      <td>6.007600</td>\n    </tr>\n    <tr>\n      <td>32800</td>\n      <td>6.005600</td>\n    </tr>\n    <tr>\n      <td>32900</td>\n      <td>6.001800</td>\n    </tr>\n    <tr>\n      <td>33000</td>\n      <td>5.987100</td>\n    </tr>\n    <tr>\n      <td>33100</td>\n      <td>5.982300</td>\n    </tr>\n    <tr>\n      <td>33200</td>\n      <td>5.978000</td>\n    </tr>\n    <tr>\n      <td>33300</td>\n      <td>6.000700</td>\n    </tr>\n    <tr>\n      <td>33400</td>\n      <td>5.991700</td>\n    </tr>\n    <tr>\n      <td>33500</td>\n      <td>5.989500</td>\n    </tr>\n    <tr>\n      <td>33600</td>\n      <td>5.991500</td>\n    </tr>\n    <tr>\n      <td>33700</td>\n      <td>6.000300</td>\n    </tr>\n    <tr>\n      <td>33800</td>\n      <td>5.988300</td>\n    </tr>\n    <tr>\n      <td>33900</td>\n      <td>5.990700</td>\n    </tr>\n    <tr>\n      <td>34000</td>\n      <td>5.989600</td>\n    </tr>\n    <tr>\n      <td>34100</td>\n      <td>5.983000</td>\n    </tr>\n    <tr>\n      <td>34200</td>\n      <td>5.982300</td>\n    </tr>\n    <tr>\n      <td>34300</td>\n      <td>5.984600</td>\n    </tr>\n    <tr>\n      <td>34400</td>\n      <td>5.977300</td>\n    </tr>\n    <tr>\n      <td>34500</td>\n      <td>5.982000</td>\n    </tr>\n    <tr>\n      <td>34600</td>\n      <td>5.986900</td>\n    </tr>\n    <tr>\n      <td>34700</td>\n      <td>5.980400</td>\n    </tr>\n    <tr>\n      <td>34800</td>\n      <td>5.943100</td>\n    </tr>\n    <tr>\n      <td>34900</td>\n      <td>5.977800</td>\n    </tr>\n    <tr>\n      <td>35000</td>\n      <td>5.983500</td>\n    </tr>\n    <tr>\n      <td>35100</td>\n      <td>5.979400</td>\n    </tr>\n    <tr>\n      <td>35200</td>\n      <td>5.980000</td>\n    </tr>\n    <tr>\n      <td>35300</td>\n      <td>5.970500</td>\n    </tr>\n    <tr>\n      <td>35400</td>\n      <td>5.979100</td>\n    </tr>\n    <tr>\n      <td>35500</td>\n      <td>5.976500</td>\n    </tr>\n    <tr>\n      <td>35600</td>\n      <td>5.976100</td>\n    </tr>\n    <tr>\n      <td>35700</td>\n      <td>5.971900</td>\n    </tr>\n    <tr>\n      <td>35800</td>\n      <td>5.974300</td>\n    </tr>\n    <tr>\n      <td>35900</td>\n      <td>5.973200</td>\n    </tr>\n    <tr>\n      <td>36000</td>\n      <td>5.964000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nos.listdir(\"/kaggle/working/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:24:55.685812Z","iopub.execute_input":"2025-08-23T15:24:55.686083Z","iopub.status.idle":"2025-08-23T15:24:55.691365Z","shell.execute_reply.started":"2025-08-23T15:24:55.686062Z","shell.execute_reply":"2025-08-23T15:24:55.690606Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"['.virtual_documents', 'checkpoints', 'merges.txt', 'vocab.json', 'logs']"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"model.save_pretrained(\"./tiny-roberta\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:26:00.390926Z","iopub.execute_input":"2025-08-23T15:26:00.391219Z","iopub.status.idle":"2025-08-23T15:26:00.448381Z","shell.execute_reply.started":"2025-08-23T15:26:00.391196Z","shell.execute_reply":"2025-08-23T15:26:00.447730Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"tokenizer.save_pretrained(\"./tiny-roberta\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:26:17.114812Z","iopub.execute_input":"2025-08-23T15:26:17.115309Z","iopub.status.idle":"2025-08-23T15:26:17.145263Z","shell.execute_reply.started":"2025-08-23T15:26:17.115288Z","shell.execute_reply":"2025-08-23T15:26:17.144653Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"('./tiny-roberta/tokenizer_config.json',\n './tiny-roberta/special_tokens_map.json',\n './tiny-roberta/vocab.json',\n './tiny-roberta/merges.txt',\n './tiny-roberta/added_tokens.json')"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:30:10.503941Z","iopub.execute_input":"2025-08-23T15:30:10.504259Z","iopub.status.idle":"2025-08-23T15:30:10.508115Z","shell.execute_reply.started":"2025-08-23T15:30:10.504237Z","shell.execute_reply":"2025-08-23T15:30:10.507402Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"test_model =  RobertaModel.from_pretrained(\"./tiny-roberta\")\ntest_tokenizer = RobertaTokenizer.from_pretrained(\"./tiny-roberta\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:31:21.927000Z","iopub.execute_input":"2025-08-23T15:31:21.927298Z","iopub.status.idle":"2025-08-23T15:31:21.993200Z","shell.execute_reply.started":"2025-08-23T15:31:21.927277Z","shell.execute_reply":"2025-08-23T15:31:21.992646Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at ./tiny-roberta and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"test_text = \"what are tariffs\"\n\ninputs = test_tokenizer(test_text, return_tensors=\"pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:31:48.663189Z","iopub.execute_input":"2025-08-23T15:31:48.663736Z","iopub.status.idle":"2025-08-23T15:31:48.667569Z","shell.execute_reply.started":"2025-08-23T15:31:48.663712Z","shell.execute_reply":"2025-08-23T15:31:48.666897Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"import torch\nwith torch.no_grad():\n    outputs = test_model(**inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:33:05.507821Z","iopub.execute_input":"2025-08-23T15:33:05.508503Z","iopub.status.idle":"2025-08-23T15:33:05.616460Z","shell.execute_reply.started":"2025-08-23T15:33:05.508479Z","shell.execute_reply":"2025-08-23T15:33:05.615753Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:33:13.215749Z","iopub.execute_input":"2025-08-23T15:33:13.216019Z","iopub.status.idle":"2025-08-23T15:33:13.229693Z","shell.execute_reply.started":"2025-08-23T15:33:13.215998Z","shell.execute_reply":"2025-08-23T15:33:13.228907Z"}},"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.8621,  0.2247,  2.4125,  ...,  0.4460, -0.4612,  0.4734],\n         [-1.1229, -0.6479,  0.5011,  ...,  0.3974,  1.0822,  0.7830],\n         [-1.2034,  0.0680,  1.7291,  ...,  1.4815, -0.0974, -0.8972],\n         ...,\n         [-1.0029,  0.7104,  0.3114,  ..., -0.7080, -0.1807, -0.3309],\n         [-0.3993,  0.8103, -0.4200,  ...,  1.1263,  1.6450, -1.1837],\n         [-1.0440,  1.0778,  1.1312,  ...,  1.5617, -0.4226,  0.1759]]]), pooler_output=tensor([[-3.3787e-01, -6.4649e-01,  3.5569e-01,  3.2207e-01,  1.5847e-01,\n          2.6981e-01,  3.0054e-01,  1.6362e-02,  3.3713e-01,  1.3520e-01,\n          3.5580e-01, -1.2581e-01,  2.2033e-02,  3.9121e-01,  1.1300e-01,\n         -3.8726e-01, -1.8054e-01,  5.3663e-01, -4.2245e-01,  3.3471e-01,\n         -3.4189e-01,  3.9718e-02,  1.4305e-01,  2.5353e-01,  5.8105e-03,\n          5.4468e-01,  2.4620e-01,  3.5678e-01, -1.6053e-01,  1.8096e-01,\n          2.1180e-01, -7.4367e-02,  5.3990e-01, -6.6182e-01,  2.1884e-02,\n         -2.2642e-02, -5.9268e-01, -3.4248e-01, -1.0206e-01, -6.9821e-01,\n          2.4914e-01, -4.2617e-01,  1.7037e-02,  5.3351e-01,  3.1755e-04,\n          1.3168e-01,  8.2996e-02,  1.2922e-01, -2.1078e-02,  3.7556e-01,\n         -1.1955e-01, -3.5340e-01,  4.3110e-01,  5.7821e-02, -4.8686e-01,\n         -1.8148e-01,  5.3535e-01,  1.2047e-01, -1.7349e-01, -5.8775e-02,\n         -5.8018e-04,  1.7775e-01, -2.7908e-01,  2.0385e-01, -3.5945e-01,\n          9.6408e-02,  1.7705e-01, -1.4805e-01, -3.2424e-01,  2.1060e-01,\n          2.7791e-01,  1.9158e-01,  4.0932e-03,  2.4068e-01, -2.5117e-01,\n         -4.9331e-02,  5.8196e-02, -1.3083e-02, -1.7495e-01,  3.2191e-01,\n          2.6379e-01,  1.8441e-01, -2.6938e-01, -3.3738e-01, -6.4075e-02,\n         -4.0201e-02,  4.1607e-01,  1.8651e-01, -5.0147e-01, -2.8504e-01,\n          2.5526e-01,  3.3142e-01,  8.6987e-02,  3.1723e-01,  3.6593e-01,\n          2.3550e-01,  4.6615e-02,  2.4164e-02, -1.7661e-01, -1.7514e-01,\n          3.9335e-02,  6.4127e-01, -1.7423e-01,  2.6592e-01, -2.8370e-01,\n         -3.7983e-01, -2.8105e-01,  4.2875e-01, -4.5329e-01,  5.5942e-01,\n         -8.0068e-01, -2.9300e-01,  2.4888e-01,  2.8189e-01, -3.1250e-01,\n         -8.6204e-02,  1.2133e-01,  3.4470e-01,  7.3801e-02, -3.1672e-01,\n         -9.7913e-02,  2.0272e-01,  1.7653e-01,  8.3866e-02, -3.2681e-01,\n          3.5925e-01,  1.5303e-01, -4.4306e-02, -1.4491e-01,  1.2148e-03,\n          1.6700e-01, -2.0869e-01, -3.2970e-01, -5.3552e-01,  6.9449e-02,\n          6.7453e-01, -1.0722e-01,  9.6590e-02, -1.8802e-01, -3.8181e-01,\n         -2.9368e-02, -3.3906e-01,  4.7112e-01, -2.0675e-01, -2.2176e-02,\n         -3.8742e-01,  1.5168e-01,  4.7507e-01,  1.8966e-02, -1.4114e-01,\n          6.7530e-02, -5.2900e-01, -2.3675e-01, -6.6140e-01,  6.1673e-01,\n          3.2529e-02, -6.0327e-02,  4.6421e-01,  2.4143e-01, -2.1361e-01,\n         -3.1634e-01, -1.6001e-01, -1.9199e-01,  2.4483e-01,  3.1457e-01,\n          1.4172e-01,  1.1506e-01,  3.1127e-02, -3.8725e-01, -2.6531e-01,\n         -6.8807e-02,  4.4741e-01, -2.5618e-01, -7.0506e-03,  3.3952e-01,\n         -2.2504e-01, -1.3037e-01,  3.0059e-01,  1.4822e-01,  3.5866e-01,\n          2.0563e-01, -2.8548e-01, -4.7720e-01,  2.6845e-01,  3.2777e-01,\n          5.9774e-01,  6.6155e-03,  2.3642e-01, -5.4398e-01,  3.8422e-02,\n         -4.9419e-01,  2.5074e-01, -4.3135e-02,  1.5397e-01,  3.0604e-01,\n         -2.5051e-01,  2.4675e-01, -1.1538e-01,  3.6675e-02,  7.5046e-03,\n         -1.4414e-01,  6.0381e-02,  2.3333e-01,  2.6326e-01, -3.9477e-01,\n         -7.8992e-02,  1.3278e-01, -6.3319e-02, -1.2733e-01, -4.5453e-01,\n          3.0052e-01,  2.4424e-01, -3.8931e-01, -3.0239e-02,  1.7146e-01,\n         -4.4637e-02,  2.3605e-01,  3.4785e-01, -5.1960e-01, -2.2387e-01,\n          1.7499e-01,  2.7314e-01, -1.7104e-01, -2.2418e-01, -3.1614e-01,\n         -5.7896e-01, -2.8936e-01,  2.5452e-01,  9.8598e-02, -7.8538e-02,\n         -3.5146e-01,  1.0754e-02,  3.9845e-01, -2.4755e-01, -2.2082e-01,\n         -6.7811e-02, -4.3575e-02, -3.4277e-01,  1.9347e-01, -2.1750e-01,\n         -1.6359e-01,  9.6409e-02,  1.4334e-01,  3.9624e-01, -1.9908e-01,\n          5.1661e-01,  2.9978e-01, -4.3514e-01, -1.4428e-01, -1.6888e-01,\n          9.6614e-02, -3.2256e-02, -3.5426e-01, -7.5314e-03, -3.8863e-01,\n          3.6780e-02]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"import torch\ndef get_mean_embed(tokenized_txt):\n    with torch.no_grad():\n        outputs_exp = test_model(**tokenized_txt)\n    last_hidden_state_exp = outputs_exp.last_hidden_state \n    cls_embedding_exp = last_hidden_state_exp[:, 0, :]     # [CLS] token representation\n    mean_embedding_exp = last_hidden_state_exp.mean(dim=1)\n    return mean_embedding_exp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:49:45.001793Z","iopub.execute_input":"2025-08-23T15:49:45.002046Z","iopub.status.idle":"2025-08-23T15:49:45.007067Z","shell.execute_reply.started":"2025-08-23T15:49:45.002028Z","shell.execute_reply":"2025-08-23T15:49:45.006215Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"def findsimilaritybetweendocs(doc1, doc2):\n    #Tokenize the docs\n    doc1_tk = test_tokenizer(doc1, return_tensors=\"pt\")\n    doc2_tk = test_tokenizer(doc2, return_tensors=\"pt\")\n    #Find the embeddings for the doc\n    doc1_emb = get_mean_embed(doc1_tk)\n    doc2_emb = get_mean_embed(doc2_tk)\n    cos_sim = F.cosine_similarity(doc1_emb, doc2_emb)\n    return cos_sim.item()\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T16:01:11.208530Z","iopub.execute_input":"2025-08-23T16:01:11.208800Z","iopub.status.idle":"2025-08-23T16:01:11.213189Z","shell.execute_reply.started":"2025-08-23T16:01:11.208779Z","shell.execute_reply":"2025-08-23T16:01:11.212365Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"findsimilaritybetweendocs(\"What is Tarif\",\"Tariffs are taxes imposed by governments on imported goods and services, used to influence trade, protect domestic industries, and generate revenue.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T16:01:15.784267Z","iopub.execute_input":"2025-08-23T16:01:15.784749Z","iopub.status.idle":"2025-08-23T16:01:15.816779Z","shell.execute_reply.started":"2025-08-23T16:01:15.784727Z","shell.execute_reply":"2025-08-23T16:01:15.816193Z"}},"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"0.8751513957977295"},"metadata":{}}],"execution_count":76},{"cell_type":"code","source":"findsimilaritybetweendocs(\"What is the capital of France?\",\"The capital of France is Paris.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T16:01:54.231702Z","iopub.execute_input":"2025-08-23T16:01:54.231958Z","iopub.status.idle":"2025-08-23T16:01:54.256810Z","shell.execute_reply.started":"2025-08-23T16:01:54.231939Z","shell.execute_reply":"2025-08-23T16:01:54.256204Z"}},"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"0.9255729913711548"},"metadata":{}}],"execution_count":80},{"cell_type":"code","source":"findsimilaritybetweendocs(\"What is the capital of France?\",\"I gIood I\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T16:12:38.737867Z","iopub.execute_input":"2025-08-23T16:12:38.738169Z","iopub.status.idle":"2025-08-23T16:12:38.764510Z","shell.execute_reply.started":"2025-08-23T16:12:38.738124Z","shell.execute_reply":"2025-08-23T16:12:38.763911Z"}},"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"0.8300344347953796"},"metadata":{}}],"execution_count":86},{"cell_type":"code","source":"import json\nimport matplotlib.pyplot as plt\n\n# Path to your trainer_state.json (inside output_dir)\nstate_file = \"./checkpoints/trainer_state.json\"\n\nwith open(state_file, \"r\") as f:\n    trainer_state = json.load(f)\n\n# Extract loss and step history\nsteps = []\nlosses = []\n\nfor log in trainer_state.get(\"log_history\", []):\n    if \"loss\" in log:\n        steps.append(log[\"step\"])\n        losses.append(log[\"loss\"])\n\n# Plot\nplt.figure(figsize=(8, 5))\nplt.plot(steps, losses, marker=\"o\")\nplt.xlabel(\"Step\")\nplt.ylabel(\"Training Loss\")\nplt.title(\"Training Loss over Steps\")\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport matplotlib.pyplot as plt\n\n# Path to your trainer_state.json\nstate_file = \"./checkpoints/trainer_state.json\"\n\nwith open(state_file, \"r\") as f:\n    trainer_state = json.load(f)\n\n# Extract steps, loss, and learning rate\nsteps = []\nlosses = []\nlrs = []\n\nfor log in trainer_state.get(\"log_history\", []):\n    if \"loss\" in log and \"learning_rate\" in log:\n        steps.append(log[\"step\"])\n        losses.append(log[\"loss\"])\n        lrs.append(log[\"learning_rate\"])\n\n# Plot with dual axes\nfig, ax1 = plt.subplots(figsize=(9, 5))\n\ncolor = \"tab:blue\"\nax1.set_xlabel(\"Step\")\nax1.set_ylabel(\"Training Loss\", color=color)\nax1.plot(steps, losses, color=color, marker=\"o\", label=\"Loss\")\nax1.tick_params(axis=\"y\", labelcolor=color)\nax1.grid(True)\n\nax2 = ax1.twinx()  # second y-axis\ncolor = \"tab:red\"\nax2.set_ylabel(\"Learning Rate\", color=color)\nax2.plot(steps, lrs, color=color, linestyle=\"--\", label=\"Learning Rate\")\nax2.tick_params(axis=\"y\", labelcolor=color)\n\nfig.suptitle(\"Training Loss and Learning Rate over Steps\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}