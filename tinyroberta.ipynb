{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12830809,"sourceType":"datasetVersion","datasetId":8114587}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-22T14:50:32.144268Z","iopub.execute_input":"2025-08-22T14:50:32.144549Z","iopub.status.idle":"2025-08-22T14:50:32.522618Z","shell.execute_reply.started":"2025-08-22T14:50:32.144527Z","shell.execute_reply":"2025-08-22T14:50:32.521351Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/wiki-cc-openweb/wiki_small_50k/wiki_small_50k.csv\n/kaggle/input/wiki-cc-openweb/openweb_50k (2)/openweb_50k.csv\n/kaggle/input/wiki-cc-openweb/cc_news/cc_news.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from datasets import load_dataset, concatenate_datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T23:50:17.569840Z","iopub.execute_input":"2025-08-23T23:50:17.570423Z","iopub.status.idle":"2025-08-23T23:50:19.562023Z","shell.execute_reply.started":"2025-08-23T23:50:17.570401Z","shell.execute_reply":"2025-08-23T23:50:19.561245Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"wiki_ds = load_dataset(\"csv\", data_files=\"/kaggle/input/wiki-cc-openweb/wiki_small_50k/wiki_small_50k.csv\")\nowt_ds = load_dataset(\"csv\", data_files=\"/kaggle/input/wiki-cc-openweb/openweb_50k (2)/openweb_50k.csv\")\ncc_news_ds = load_dataset(\"csv\", data_files=\"/kaggle/input/wiki-cc-openweb/cc_news/cc_news.csv\")\ntrain_ds = concatenate_datasets([wiki_ds[\"train\"],owt_ds[\"train\"],cc_news_ds[\"train\"]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T23:50:22.180910Z","iopub.execute_input":"2025-08-23T23:50:22.181320Z","iopub.status.idle":"2025-08-23T23:50:36.533021Z","shell.execute_reply.started":"2025-08-23T23:50:22.181289Z","shell.execute_reply":"2025-08-23T23:50:36.532452Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7d5809353f8446a979868d2854b00fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0ab03b2707e4be7b3e0a0bbf6b5c349"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbc3b7c957574bf784e8a13e99970bd6"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from tokenizers import ByteLevelBPETokenizer\nspecial_tokens = ['<s>','<pad>','</s>','<unk>','<mask>']\ntokenizer = ByteLevelBPETokenizer()\ntokenizer.train_from_iterator([x['text'] for x in train_ds], vocab_size = 10_000,min_frequency =2, special_tokens=special_tokens)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T23:50:47.018111Z","iopub.execute_input":"2025-08-23T23:50:47.018970Z","iopub.status.idle":"2025-08-23T23:52:19.401138Z","shell.execute_reply.started":"2025-08-23T23:50:47.018943Z","shell.execute_reply":"2025-08-23T23:52:19.400518Z"}},"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"tokenizer.save_model(\"/kaggle/working/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T23:53:05.543504Z","iopub.execute_input":"2025-08-23T23:53:05.544115Z","iopub.status.idle":"2025-08-23T23:53:05.555351Z","shell.execute_reply.started":"2025-08-23T23:53:05.544091Z","shell.execute_reply":"2025-08-23T23:53:05.554645Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/vocab.json', '/kaggle/working/merges.txt']"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from transformers import RobertaTokenizer\ntokenizer = RobertaTokenizer.from_pretrained(\"/kaggle/working/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T23:53:08.115361Z","iopub.execute_input":"2025-08-23T23:53:08.115647Z","iopub.status.idle":"2025-08-23T23:53:18.974108Z","shell.execute_reply.started":"2025-08-23T23:53:08.115624Z","shell.execute_reply":"2025-08-23T23:53:18.973303Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"indices = [0, 2]  # Example indices\nsubset = train_ds.select(indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T14:55:41.894197Z","iopub.execute_input":"2025-08-23T14:55:41.894615Z","iopub.status.idle":"2025-08-23T14:55:41.904805Z","shell.execute_reply.started":"2025-08-23T14:55:41.894580Z","shell.execute_reply":"2025-08-23T14:55:41.903913Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"type(subset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T14:55:43.801822Z","iopub.execute_input":"2025-08-23T14:55:43.802446Z","iopub.status.idle":"2025-08-23T14:55:43.807308Z","shell.execute_reply.started":"2025-08-23T14:55:43.802420Z","shell.execute_reply":"2025-08-23T14:55:43.806525Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"datasets.arrow_dataset.Dataset"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"def tokenize(batch):\n    result = tokenizer(batch[\"text\"], truncation=True, max_length=254,return_tensors=\"pt\" )\n    print(result)\n    result = {\"input_ids\": result[\"input_ids\"].squeeze(0),\n    \"attention_mask\":result[\"attention_mask\"].squeeze(0)}\n    print(result)\n    return result\n\ntrain_dataset = subset.map(tokenize, remove_columns=[\"text\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:02:18.283107Z","iopub.execute_input":"2025-08-23T15:02:18.283400Z","iopub.status.idle":"2025-08-23T15:02:18.483442Z","shell.execute_reply.started":"2025-08-23T15:02:18.283379Z","shell.execute_reply":"2025-08-23T15:02:18.482668Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a61457829fe406f9764e43828261379"}},"metadata":{}},{"name":"stdout","text":"{'input_ids': tensor([[   0, 9634, 2278, 4204, 2374, 7303, 2278, 4204,  365,  809, 1270,  836,\n         5215,  688, 1275, 1939, 2763,   25,   13,  352,  368, 2743, 2875,  400,\n           18,  203,  203, 7994,  203, 1238, 7102,  333,  567,  559,  353, 9225,\n          784,   83,  348,   77, 1486,   77,   16,  333, 4428,  347, 1171,  325,\n         1325,  365,   88,  875,  444, 1274,  288, 1350, 7470,  660, 1309, 3409,\n          729,  295,  352,  444, 6709,  601,  286,  457,  303,   18,  631, 1743,\n          327,  291, 4820,  376,  267, 4284, 4826,  290, 4193,   16, 4284, 2721,\n          290, 4193,  295, 2271,  409, 2721,   16, 7418,   31,  444, 2131, 2533,\n          373,  309,  828, 3553, 4864,  295, 1405,  271,   86,  499, 1754, 6139,\n           16,  515, 1141, 1720, 7248, 4725,  353, 7303, 2102, 4849,   18,  631,\n         1791,  353, 1904,  264,  400, 5550, 1057,  603,  335,   16,  295, 4802,\n          384, 2435, 5256,   83,  353, 1904,  264,  400,  315,  363, 1777, 8506,\n         2655,   77,  295,  286,  488,  400,  315, 1045,   69, 4586, 3584,   18,\n          631, 1074,  749,  348,   77, 1486,   77,  409, 3875,   86, 1076,   16,\n          353, 2710,  510,  355,  291,  787,  444, 1342, 6038, 6009,  784,  686,\n          291,  343, 1791,   18,  203,  203, 2025, 1852,  203, 2374, 2278, 4204,\n          203,  384,  540, 1660,  266,   71, 5870,  295,  267, 5578, 4420,  412,\n          512,   30,  836,  809,   17,  706,  809,   30, 2074, 2337,  713,  315,\n         4068, 2999,  290, 4800, 1217,  314, 5870, 2020,  316, 1978,  327, 5578,\n         4420, 1195, 5830,  509,  295,  315,  390, 3716,  813,  203,  203, 1257,\n         5215, 3014,  203,  706, 3589, 3578,  203,  809,  380,   17, 3199, 1446,\n          595,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n{'input_ids': tensor([   0, 9634, 2278, 4204, 2374, 7303, 2278, 4204,  365,  809, 1270,  836,\n        5215,  688, 1275, 1939, 2763,   25,   13,  352,  368, 2743, 2875,  400,\n          18,  203,  203, 7994,  203, 1238, 7102,  333,  567,  559,  353, 9225,\n         784,   83,  348,   77, 1486,   77,   16,  333, 4428,  347, 1171,  325,\n        1325,  365,   88,  875,  444, 1274,  288, 1350, 7470,  660, 1309, 3409,\n         729,  295,  352,  444, 6709,  601,  286,  457,  303,   18,  631, 1743,\n         327,  291, 4820,  376,  267, 4284, 4826,  290, 4193,   16, 4284, 2721,\n         290, 4193,  295, 2271,  409, 2721,   16, 7418,   31,  444, 2131, 2533,\n         373,  309,  828, 3553, 4864,  295, 1405,  271,   86,  499, 1754, 6139,\n          16,  515, 1141, 1720, 7248, 4725,  353, 7303, 2102, 4849,   18,  631,\n        1791,  353, 1904,  264,  400, 5550, 1057,  603,  335,   16,  295, 4802,\n         384, 2435, 5256,   83,  353, 1904,  264,  400,  315,  363, 1777, 8506,\n        2655,   77,  295,  286,  488,  400,  315, 1045,   69, 4586, 3584,   18,\n         631, 1074,  749,  348,   77, 1486,   77,  409, 3875,   86, 1076,   16,\n         353, 2710,  510,  355,  291,  787,  444, 1342, 6038, 6009,  784,  686,\n         291,  343, 1791,   18,  203,  203, 2025, 1852,  203, 2374, 2278, 4204,\n         203,  384,  540, 1660,  266,   71, 5870,  295,  267, 5578, 4420,  412,\n         512,   30,  836,  809,   17,  706,  809,   30, 2074, 2337,  713,  315,\n        4068, 2999,  290, 4800, 1217,  314, 5870, 2020,  316, 1978,  327, 5578,\n        4420, 1195, 5830,  509,  295,  315,  390, 3716,  813,  203,  203, 1257,\n        5215, 3014,  203,  706, 3589, 3578,  203,  809,  380,   17, 3199, 1446,\n         595,    2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n{'input_ids': tensor([[   0,   48,  351, 4637,  488,  382, 1993, 3317,  277,  469,  384,  382,\n          351, 4637,  488,  382, 1993, 3317,  277,  469, 5450,  352,  262, 2175,\n         4948,  290,  267, 3967,  364,  889,   93,  290,  382,  351, 4637,  469,\n           18,  203,  203, 4482,  225,  203,  432, 4948,  352, 2574,  276,  288,\n         7268,   72, 4064,   18,  203,  203, 2869, 6913,  225,  203,  432, 6397,\n          469,  352,  297, 2615,  288,  262, 7377, 1246,   84,   93, 2364, 8823,\n          353, 5310, 1478,  663,  295, 3865,  511,   88,  694,   18,  203,  203,\n         6742,  383,  361,  203,  203,   38, 9561, 2623,  203,  203,   39,  277,\n          586,  203,  203, 1229,  991,  203, 8589,  383,  364,  889,   93,  290,\n          382,  351, 4637,  469,  203,  203, 9573, 1814, 5061,  295, 1408,  586,\n          634, 9651, 2700,  288, 1419, 9342,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1]])}\n{'input_ids': tensor([   0,   48,  351, 4637,  488,  382, 1993, 3317,  277,  469,  384,  382,\n         351, 4637,  488,  382, 1993, 3317,  277,  469, 5450,  352,  262, 2175,\n        4948,  290,  267, 3967,  364,  889,   93,  290,  382,  351, 4637,  469,\n          18,  203,  203, 4482,  225,  203,  432, 4948,  352, 2574,  276,  288,\n        7268,   72, 4064,   18,  203,  203, 2869, 6913,  225,  203,  432, 6397,\n         469,  352,  297, 2615,  288,  262, 7377, 1246,   84,   93, 2364, 8823,\n         353, 5310, 1478,  663,  295, 3865,  511,   88,  694,   18,  203,  203,\n        6742,  383,  361,  203,  203,   38, 9561, 2623,  203,  203,   39,  277,\n         586,  203,  203, 1229,  991,  203, 8589,  383,  364,  889,   93,  290,\n         382,  351, 4637,  469,  203,  203, 9573, 1814, 5061,  295, 1408,  586,\n         634, 9651, 2700,  288, 1419, 9342,    2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1])}\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"train_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T14:53:53.405928Z","iopub.execute_input":"2025-08-23T14:53:53.406253Z","iopub.status.idle":"2025-08-23T14:53:53.411678Z","shell.execute_reply.started":"2025-08-23T14:53:53.406231Z","shell.execute_reply":"2025-08-23T14:53:53.410953Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask'],\n    num_rows: 5\n})"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# Real text\n\n\nsample_dataset = []\nfor text in train_ds['text']:\n    enc = tokenizer(text, truncation=True, max_length=254, return_tensors=\"pt\")\n    sample_dataset.append({\n        \"input_ids\": enc[\"input_ids\"].squeeze(0),\n        \"attention_mask\": enc[\"attention_mask\"].squeeze(0)\n    })\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T00:07:20.542443Z","iopub.execute_input":"2025-08-24T00:07:20.543068Z","iopub.status.idle":"2025-08-24T00:20:55.806302Z","shell.execute_reply.started":"2025-08-24T00:07:20.543048Z","shell.execute_reply":"2025-08-24T00:20:55.805695Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from transformers import (\n    RobertaConfig,\n    RobertaForMaskedLM,\n\n    DataCollatorForLanguageModeling,\n    Trainer,\n    TrainingArguments,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T00:21:18.944568Z","iopub.execute_input":"2025-08-24T00:21:18.944842Z","iopub.status.idle":"2025-08-24T00:21:18.976510Z","shell.execute_reply.started":"2025-08-24T00:21:18.944824Z","shell.execute_reply":"2025-08-24T00:21:18.975955Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"config = RobertaConfig(\nvocab_size = tokenizer.vocab_size,\nhidden_size = 256, #Embedding size\nnum_hidden_layers = 6,#Number of encoder layers\nnum_attention_heads=4, # Attentions per encoder layer, divisible by 256\nintermediate_size = 1024, #FFN layer size\nmax_position_embeddings=256, #Position Embedding max size = embedding size\ntype_vocab_size =1,\n    \nlayer_norm_eps = 1e-5,\ninitializer_range = 0.02,\npad_token_id = 1,\nbos_token_id = 0,\neos_token_id = 2\n)\nmodel = RobertaForMaskedLM(config)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T00:21:24.191600Z","iopub.execute_input":"2025-08-24T00:21:24.191942Z","iopub.status.idle":"2025-08-24T00:21:24.434376Z","shell.execute_reply.started":"2025-08-24T00:21:24.191920Z","shell.execute_reply":"2025-08-24T00:21:24.433779Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"data_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T00:21:27.520026Z","iopub.execute_input":"2025-08-24T00:21:27.520305Z","iopub.status.idle":"2025-08-24T00:21:27.524023Z","shell.execute_reply.started":"2025-08-24T00:21:27.520285Z","shell.execute_reply":"2025-08-24T00:21:27.523339Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# 4️⃣ Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./checkpoints\",\n    overwrite_output_dir = False,\n    do_train=True,\n    save_steps=100,\n    logging_steps=100,\n    num_train_epochs=15,\n    per_device_train_batch_size=16,\n    gradient_accumulation_steps=2,\n    warmup_steps = 100,\n    logging_dir = \"./logs\",\n    fp16=True,\n    save_total_limit=3,\n    save_strategy = \"steps\",\n    report_to=[\"tensorboard\"]\n)\n\n# 5️⃣ Create Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=sample_dataset,\n    data_collator=data_collator\n)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T00:21:33.968145Z","iopub.execute_input":"2025-08-24T00:21:33.968635Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='20946' max='40050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [20946/40050 1:19:34 < 1:12:34, 4.39 it/s, Epoch 7.84/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>8.940000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>8.092200</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>7.571200</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>7.390900</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>7.341000</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>7.301600</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>7.269600</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>7.233900</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>7.199000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>7.175100</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>7.155100</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>7.135100</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>7.121300</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>7.098300</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>7.074900</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>7.061300</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>7.033500</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>7.019500</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>7.007300</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>6.983400</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>6.977200</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>6.963200</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>6.941000</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>6.932900</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>6.914800</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>6.903100</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>6.865600</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>6.886700</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>6.876400</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>6.880100</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>6.859900</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>6.855100</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>6.840500</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>6.831200</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>6.827100</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>6.823100</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>6.813400</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>6.800000</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>6.797200</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>6.796300</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>6.777200</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>6.784300</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>6.769400</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>6.750700</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>6.747200</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>6.747000</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>6.742200</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>6.731000</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>6.731600</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>6.712900</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>6.728000</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>6.710000</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>6.702500</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>6.659700</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>6.692700</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>6.695900</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>6.686400</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>6.671700</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>6.661900</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>6.651900</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>6.661700</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>6.650000</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>6.650300</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>6.639000</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>6.636500</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>6.637000</td>\n    </tr>\n    <tr>\n      <td>6700</td>\n      <td>6.615400</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>6.626300</td>\n    </tr>\n    <tr>\n      <td>6900</td>\n      <td>6.610900</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>6.607600</td>\n    </tr>\n    <tr>\n      <td>7100</td>\n      <td>6.599900</td>\n    </tr>\n    <tr>\n      <td>7200</td>\n      <td>6.597400</td>\n    </tr>\n    <tr>\n      <td>7300</td>\n      <td>6.593100</td>\n    </tr>\n    <tr>\n      <td>7400</td>\n      <td>6.587200</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>6.591800</td>\n    </tr>\n    <tr>\n      <td>7600</td>\n      <td>6.581900</td>\n    </tr>\n    <tr>\n      <td>7700</td>\n      <td>6.574900</td>\n    </tr>\n    <tr>\n      <td>7800</td>\n      <td>6.572400</td>\n    </tr>\n    <tr>\n      <td>7900</td>\n      <td>6.570000</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>6.554200</td>\n    </tr>\n    <tr>\n      <td>8100</td>\n      <td>6.517800</td>\n    </tr>\n    <tr>\n      <td>8200</td>\n      <td>6.550700</td>\n    </tr>\n    <tr>\n      <td>8300</td>\n      <td>6.545600</td>\n    </tr>\n    <tr>\n      <td>8400</td>\n      <td>6.552700</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>6.535200</td>\n    </tr>\n    <tr>\n      <td>8600</td>\n      <td>6.534700</td>\n    </tr>\n    <tr>\n      <td>8700</td>\n      <td>6.548000</td>\n    </tr>\n    <tr>\n      <td>8800</td>\n      <td>6.524500</td>\n    </tr>\n    <tr>\n      <td>8900</td>\n      <td>6.539200</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>6.534000</td>\n    </tr>\n    <tr>\n      <td>9100</td>\n      <td>6.526600</td>\n    </tr>\n    <tr>\n      <td>9200</td>\n      <td>6.520400</td>\n    </tr>\n    <tr>\n      <td>9300</td>\n      <td>6.511900</td>\n    </tr>\n    <tr>\n      <td>9400</td>\n      <td>6.509500</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>6.493000</td>\n    </tr>\n    <tr>\n      <td>9600</td>\n      <td>6.507500</td>\n    </tr>\n    <tr>\n      <td>9700</td>\n      <td>6.507100</td>\n    </tr>\n    <tr>\n      <td>9800</td>\n      <td>6.484700</td>\n    </tr>\n    <tr>\n      <td>9900</td>\n      <td>6.487400</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>6.496500</td>\n    </tr>\n    <tr>\n      <td>10100</td>\n      <td>6.456300</td>\n    </tr>\n    <tr>\n      <td>10200</td>\n      <td>6.477600</td>\n    </tr>\n    <tr>\n      <td>10300</td>\n      <td>6.477900</td>\n    </tr>\n    <tr>\n      <td>10400</td>\n      <td>6.461700</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>6.475200</td>\n    </tr>\n    <tr>\n      <td>10600</td>\n      <td>6.460400</td>\n    </tr>\n    <tr>\n      <td>10700</td>\n      <td>6.432900</td>\n    </tr>\n    <tr>\n      <td>10800</td>\n      <td>6.452800</td>\n    </tr>\n    <tr>\n      <td>10900</td>\n      <td>6.453200</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>6.455100</td>\n    </tr>\n    <tr>\n      <td>11100</td>\n      <td>6.447100</td>\n    </tr>\n    <tr>\n      <td>11200</td>\n      <td>6.442000</td>\n    </tr>\n    <tr>\n      <td>11300</td>\n      <td>6.429700</td>\n    </tr>\n    <tr>\n      <td>11400</td>\n      <td>6.441800</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>6.430800</td>\n    </tr>\n    <tr>\n      <td>11600</td>\n      <td>6.435200</td>\n    </tr>\n    <tr>\n      <td>11700</td>\n      <td>6.423200</td>\n    </tr>\n    <tr>\n      <td>11800</td>\n      <td>6.423100</td>\n    </tr>\n    <tr>\n      <td>11900</td>\n      <td>6.425100</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>6.422900</td>\n    </tr>\n    <tr>\n      <td>12100</td>\n      <td>6.420500</td>\n    </tr>\n    <tr>\n      <td>12200</td>\n      <td>6.416700</td>\n    </tr>\n    <tr>\n      <td>12300</td>\n      <td>6.417500</td>\n    </tr>\n    <tr>\n      <td>12400</td>\n      <td>6.421700</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>6.403600</td>\n    </tr>\n    <tr>\n      <td>12600</td>\n      <td>6.404700</td>\n    </tr>\n    <tr>\n      <td>12700</td>\n      <td>6.401700</td>\n    </tr>\n    <tr>\n      <td>12800</td>\n      <td>6.392400</td>\n    </tr>\n    <tr>\n      <td>12900</td>\n      <td>6.394900</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>6.395700</td>\n    </tr>\n    <tr>\n      <td>13100</td>\n      <td>6.394400</td>\n    </tr>\n    <tr>\n      <td>13200</td>\n      <td>6.390800</td>\n    </tr>\n    <tr>\n      <td>13300</td>\n      <td>6.389000</td>\n    </tr>\n    <tr>\n      <td>13400</td>\n      <td>6.360200</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>6.377300</td>\n    </tr>\n    <tr>\n      <td>13600</td>\n      <td>6.377100</td>\n    </tr>\n    <tr>\n      <td>13700</td>\n      <td>6.354200</td>\n    </tr>\n    <tr>\n      <td>13800</td>\n      <td>6.373000</td>\n    </tr>\n    <tr>\n      <td>13900</td>\n      <td>6.372500</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>6.370000</td>\n    </tr>\n    <tr>\n      <td>14100</td>\n      <td>6.352200</td>\n    </tr>\n    <tr>\n      <td>14200</td>\n      <td>6.359000</td>\n    </tr>\n    <tr>\n      <td>14300</td>\n      <td>6.351600</td>\n    </tr>\n    <tr>\n      <td>14400</td>\n      <td>6.350000</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>6.349800</td>\n    </tr>\n    <tr>\n      <td>14600</td>\n      <td>6.352800</td>\n    </tr>\n    <tr>\n      <td>14700</td>\n      <td>6.346100</td>\n    </tr>\n    <tr>\n      <td>14800</td>\n      <td>6.337000</td>\n    </tr>\n    <tr>\n      <td>14900</td>\n      <td>6.340000</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>6.339800</td>\n    </tr>\n    <tr>\n      <td>15100</td>\n      <td>6.340400</td>\n    </tr>\n    <tr>\n      <td>15200</td>\n      <td>6.324000</td>\n    </tr>\n    <tr>\n      <td>15300</td>\n      <td>6.328900</td>\n    </tr>\n    <tr>\n      <td>15400</td>\n      <td>6.329700</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>6.330700</td>\n    </tr>\n    <tr>\n      <td>15600</td>\n      <td>6.320900</td>\n    </tr>\n    <tr>\n      <td>15700</td>\n      <td>6.320400</td>\n    </tr>\n    <tr>\n      <td>15800</td>\n      <td>6.324800</td>\n    </tr>\n    <tr>\n      <td>15900</td>\n      <td>6.324100</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>6.309300</td>\n    </tr>\n    <tr>\n      <td>16100</td>\n      <td>6.276100</td>\n    </tr>\n    <tr>\n      <td>16200</td>\n      <td>6.294000</td>\n    </tr>\n    <tr>\n      <td>16300</td>\n      <td>6.293500</td>\n    </tr>\n    <tr>\n      <td>16400</td>\n      <td>6.304100</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>6.287300</td>\n    </tr>\n    <tr>\n      <td>16600</td>\n      <td>6.283300</td>\n    </tr>\n    <tr>\n      <td>16700</td>\n      <td>6.302000</td>\n    </tr>\n    <tr>\n      <td>16800</td>\n      <td>6.277800</td>\n    </tr>\n    <tr>\n      <td>16900</td>\n      <td>6.287400</td>\n    </tr>\n    <tr>\n      <td>17000</td>\n      <td>6.291400</td>\n    </tr>\n    <tr>\n      <td>17100</td>\n      <td>6.288400</td>\n    </tr>\n    <tr>\n      <td>17200</td>\n      <td>6.284400</td>\n    </tr>\n    <tr>\n      <td>17300</td>\n      <td>6.283400</td>\n    </tr>\n    <tr>\n      <td>17400</td>\n      <td>6.270200</td>\n    </tr>\n    <tr>\n      <td>17500</td>\n      <td>6.269000</td>\n    </tr>\n    <tr>\n      <td>17600</td>\n      <td>6.257300</td>\n    </tr>\n    <tr>\n      <td>17700</td>\n      <td>6.258600</td>\n    </tr>\n    <tr>\n      <td>17800</td>\n      <td>6.262700</td>\n    </tr>\n    <tr>\n      <td>17900</td>\n      <td>6.248200</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>6.258600</td>\n    </tr>\n    <tr>\n      <td>18100</td>\n      <td>6.258300</td>\n    </tr>\n    <tr>\n      <td>18200</td>\n      <td>6.251700</td>\n    </tr>\n    <tr>\n      <td>18300</td>\n      <td>6.260200</td>\n    </tr>\n    <tr>\n      <td>18400</td>\n      <td>6.248900</td>\n    </tr>\n    <tr>\n      <td>18500</td>\n      <td>6.242200</td>\n    </tr>\n    <tr>\n      <td>18600</td>\n      <td>6.246100</td>\n    </tr>\n    <tr>\n      <td>18700</td>\n      <td>6.211900</td>\n    </tr>\n    <tr>\n      <td>18800</td>\n      <td>6.239300</td>\n    </tr>\n    <tr>\n      <td>18900</td>\n      <td>6.235300</td>\n    </tr>\n    <tr>\n      <td>19000</td>\n      <td>6.228100</td>\n    </tr>\n    <tr>\n      <td>19100</td>\n      <td>6.233600</td>\n    </tr>\n    <tr>\n      <td>19200</td>\n      <td>6.234700</td>\n    </tr>\n    <tr>\n      <td>19300</td>\n      <td>6.230100</td>\n    </tr>\n    <tr>\n      <td>19400</td>\n      <td>6.217100</td>\n    </tr>\n    <tr>\n      <td>19500</td>\n      <td>6.217500</td>\n    </tr>\n    <tr>\n      <td>19600</td>\n      <td>6.220600</td>\n    </tr>\n    <tr>\n      <td>19700</td>\n      <td>6.200700</td>\n    </tr>\n    <tr>\n      <td>19800</td>\n      <td>6.212500</td>\n    </tr>\n    <tr>\n      <td>19900</td>\n      <td>6.213700</td>\n    </tr>\n    <tr>\n      <td>20000</td>\n      <td>6.204800</td>\n    </tr>\n    <tr>\n      <td>20100</td>\n      <td>6.204300</td>\n    </tr>\n    <tr>\n      <td>20200</td>\n      <td>6.205400</td>\n    </tr>\n    <tr>\n      <td>20300</td>\n      <td>6.200200</td>\n    </tr>\n    <tr>\n      <td>20400</td>\n      <td>6.193900</td>\n    </tr>\n    <tr>\n      <td>20500</td>\n      <td>6.190300</td>\n    </tr>\n    <tr>\n      <td>20600</td>\n      <td>6.191800</td>\n    </tr>\n    <tr>\n      <td>20700</td>\n      <td>6.180900</td>\n    </tr>\n    <tr>\n      <td>20800</td>\n      <td>6.190400</td>\n    </tr>\n    <tr>\n      <td>20900</td>\n      <td>6.185900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nos.listdir(\"/kaggle/working/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T23:50:07.255473Z","iopub.execute_input":"2025-08-23T23:50:07.255720Z","iopub.status.idle":"2025-08-23T23:50:07.265328Z","shell.execute_reply.started":"2025-08-23T23:50:07.255692Z","shell.execute_reply":"2025-08-23T23:50:07.264534Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"['.virtual_documents']"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"model.save_pretrained(\"./tiny-roberta\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:26:00.390926Z","iopub.execute_input":"2025-08-23T15:26:00.391219Z","iopub.status.idle":"2025-08-23T15:26:00.448381Z","shell.execute_reply.started":"2025-08-23T15:26:00.391196Z","shell.execute_reply":"2025-08-23T15:26:00.447730Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"tokenizer.save_pretrained(\"./tiny-roberta\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:26:17.114812Z","iopub.execute_input":"2025-08-23T15:26:17.115309Z","iopub.status.idle":"2025-08-23T15:26:17.145263Z","shell.execute_reply.started":"2025-08-23T15:26:17.115288Z","shell.execute_reply":"2025-08-23T15:26:17.144653Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"('./tiny-roberta/tokenizer_config.json',\n './tiny-roberta/special_tokens_map.json',\n './tiny-roberta/vocab.json',\n './tiny-roberta/merges.txt',\n './tiny-roberta/added_tokens.json')"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:30:10.503941Z","iopub.execute_input":"2025-08-23T15:30:10.504259Z","iopub.status.idle":"2025-08-23T15:30:10.508115Z","shell.execute_reply.started":"2025-08-23T15:30:10.504237Z","shell.execute_reply":"2025-08-23T15:30:10.507402Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"test_model =  RobertaModel.from_pretrained(\"./tiny-roberta\")\ntest_tokenizer = RobertaTokenizer.from_pretrained(\"./tiny-roberta\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:31:21.927000Z","iopub.execute_input":"2025-08-23T15:31:21.927298Z","iopub.status.idle":"2025-08-23T15:31:21.993200Z","shell.execute_reply.started":"2025-08-23T15:31:21.927277Z","shell.execute_reply":"2025-08-23T15:31:21.992646Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at ./tiny-roberta and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"test_text = \"what are tariffs\"\n\ninputs = test_tokenizer(test_text, return_tensors=\"pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:31:48.663189Z","iopub.execute_input":"2025-08-23T15:31:48.663736Z","iopub.status.idle":"2025-08-23T15:31:48.667569Z","shell.execute_reply.started":"2025-08-23T15:31:48.663712Z","shell.execute_reply":"2025-08-23T15:31:48.666897Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"import torch\nwith torch.no_grad():\n    outputs = test_model(**inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:33:05.507821Z","iopub.execute_input":"2025-08-23T15:33:05.508503Z","iopub.status.idle":"2025-08-23T15:33:05.616460Z","shell.execute_reply.started":"2025-08-23T15:33:05.508479Z","shell.execute_reply":"2025-08-23T15:33:05.615753Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:33:13.215749Z","iopub.execute_input":"2025-08-23T15:33:13.216019Z","iopub.status.idle":"2025-08-23T15:33:13.229693Z","shell.execute_reply.started":"2025-08-23T15:33:13.215998Z","shell.execute_reply":"2025-08-23T15:33:13.228907Z"}},"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.8621,  0.2247,  2.4125,  ...,  0.4460, -0.4612,  0.4734],\n         [-1.1229, -0.6479,  0.5011,  ...,  0.3974,  1.0822,  0.7830],\n         [-1.2034,  0.0680,  1.7291,  ...,  1.4815, -0.0974, -0.8972],\n         ...,\n         [-1.0029,  0.7104,  0.3114,  ..., -0.7080, -0.1807, -0.3309],\n         [-0.3993,  0.8103, -0.4200,  ...,  1.1263,  1.6450, -1.1837],\n         [-1.0440,  1.0778,  1.1312,  ...,  1.5617, -0.4226,  0.1759]]]), pooler_output=tensor([[-3.3787e-01, -6.4649e-01,  3.5569e-01,  3.2207e-01,  1.5847e-01,\n          2.6981e-01,  3.0054e-01,  1.6362e-02,  3.3713e-01,  1.3520e-01,\n          3.5580e-01, -1.2581e-01,  2.2033e-02,  3.9121e-01,  1.1300e-01,\n         -3.8726e-01, -1.8054e-01,  5.3663e-01, -4.2245e-01,  3.3471e-01,\n         -3.4189e-01,  3.9718e-02,  1.4305e-01,  2.5353e-01,  5.8105e-03,\n          5.4468e-01,  2.4620e-01,  3.5678e-01, -1.6053e-01,  1.8096e-01,\n          2.1180e-01, -7.4367e-02,  5.3990e-01, -6.6182e-01,  2.1884e-02,\n         -2.2642e-02, -5.9268e-01, -3.4248e-01, -1.0206e-01, -6.9821e-01,\n          2.4914e-01, -4.2617e-01,  1.7037e-02,  5.3351e-01,  3.1755e-04,\n          1.3168e-01,  8.2996e-02,  1.2922e-01, -2.1078e-02,  3.7556e-01,\n         -1.1955e-01, -3.5340e-01,  4.3110e-01,  5.7821e-02, -4.8686e-01,\n         -1.8148e-01,  5.3535e-01,  1.2047e-01, -1.7349e-01, -5.8775e-02,\n         -5.8018e-04,  1.7775e-01, -2.7908e-01,  2.0385e-01, -3.5945e-01,\n          9.6408e-02,  1.7705e-01, -1.4805e-01, -3.2424e-01,  2.1060e-01,\n          2.7791e-01,  1.9158e-01,  4.0932e-03,  2.4068e-01, -2.5117e-01,\n         -4.9331e-02,  5.8196e-02, -1.3083e-02, -1.7495e-01,  3.2191e-01,\n          2.6379e-01,  1.8441e-01, -2.6938e-01, -3.3738e-01, -6.4075e-02,\n         -4.0201e-02,  4.1607e-01,  1.8651e-01, -5.0147e-01, -2.8504e-01,\n          2.5526e-01,  3.3142e-01,  8.6987e-02,  3.1723e-01,  3.6593e-01,\n          2.3550e-01,  4.6615e-02,  2.4164e-02, -1.7661e-01, -1.7514e-01,\n          3.9335e-02,  6.4127e-01, -1.7423e-01,  2.6592e-01, -2.8370e-01,\n         -3.7983e-01, -2.8105e-01,  4.2875e-01, -4.5329e-01,  5.5942e-01,\n         -8.0068e-01, -2.9300e-01,  2.4888e-01,  2.8189e-01, -3.1250e-01,\n         -8.6204e-02,  1.2133e-01,  3.4470e-01,  7.3801e-02, -3.1672e-01,\n         -9.7913e-02,  2.0272e-01,  1.7653e-01,  8.3866e-02, -3.2681e-01,\n          3.5925e-01,  1.5303e-01, -4.4306e-02, -1.4491e-01,  1.2148e-03,\n          1.6700e-01, -2.0869e-01, -3.2970e-01, -5.3552e-01,  6.9449e-02,\n          6.7453e-01, -1.0722e-01,  9.6590e-02, -1.8802e-01, -3.8181e-01,\n         -2.9368e-02, -3.3906e-01,  4.7112e-01, -2.0675e-01, -2.2176e-02,\n         -3.8742e-01,  1.5168e-01,  4.7507e-01,  1.8966e-02, -1.4114e-01,\n          6.7530e-02, -5.2900e-01, -2.3675e-01, -6.6140e-01,  6.1673e-01,\n          3.2529e-02, -6.0327e-02,  4.6421e-01,  2.4143e-01, -2.1361e-01,\n         -3.1634e-01, -1.6001e-01, -1.9199e-01,  2.4483e-01,  3.1457e-01,\n          1.4172e-01,  1.1506e-01,  3.1127e-02, -3.8725e-01, -2.6531e-01,\n         -6.8807e-02,  4.4741e-01, -2.5618e-01, -7.0506e-03,  3.3952e-01,\n         -2.2504e-01, -1.3037e-01,  3.0059e-01,  1.4822e-01,  3.5866e-01,\n          2.0563e-01, -2.8548e-01, -4.7720e-01,  2.6845e-01,  3.2777e-01,\n          5.9774e-01,  6.6155e-03,  2.3642e-01, -5.4398e-01,  3.8422e-02,\n         -4.9419e-01,  2.5074e-01, -4.3135e-02,  1.5397e-01,  3.0604e-01,\n         -2.5051e-01,  2.4675e-01, -1.1538e-01,  3.6675e-02,  7.5046e-03,\n         -1.4414e-01,  6.0381e-02,  2.3333e-01,  2.6326e-01, -3.9477e-01,\n         -7.8992e-02,  1.3278e-01, -6.3319e-02, -1.2733e-01, -4.5453e-01,\n          3.0052e-01,  2.4424e-01, -3.8931e-01, -3.0239e-02,  1.7146e-01,\n         -4.4637e-02,  2.3605e-01,  3.4785e-01, -5.1960e-01, -2.2387e-01,\n          1.7499e-01,  2.7314e-01, -1.7104e-01, -2.2418e-01, -3.1614e-01,\n         -5.7896e-01, -2.8936e-01,  2.5452e-01,  9.8598e-02, -7.8538e-02,\n         -3.5146e-01,  1.0754e-02,  3.9845e-01, -2.4755e-01, -2.2082e-01,\n         -6.7811e-02, -4.3575e-02, -3.4277e-01,  1.9347e-01, -2.1750e-01,\n         -1.6359e-01,  9.6409e-02,  1.4334e-01,  3.9624e-01, -1.9908e-01,\n          5.1661e-01,  2.9978e-01, -4.3514e-01, -1.4428e-01, -1.6888e-01,\n          9.6614e-02, -3.2256e-02, -3.5426e-01, -7.5314e-03, -3.8863e-01,\n          3.6780e-02]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"import torch\ndef get_mean_embed(tokenized_txt):\n    with torch.no_grad():\n        outputs_exp = test_model(**tokenized_txt)\n    last_hidden_state_exp = outputs_exp.last_hidden_state \n    cls_embedding_exp = last_hidden_state_exp[:, 0, :]     # [CLS] token representation\n    mean_embedding_exp = last_hidden_state_exp.mean(dim=1)\n    return mean_embedding_exp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T15:49:45.001793Z","iopub.execute_input":"2025-08-23T15:49:45.002046Z","iopub.status.idle":"2025-08-23T15:49:45.007067Z","shell.execute_reply.started":"2025-08-23T15:49:45.002028Z","shell.execute_reply":"2025-08-23T15:49:45.006215Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"def findsimilaritybetweendocs(doc1, doc2):\n    #Tokenize the docs\n    doc1_tk = test_tokenizer(doc1, return_tensors=\"pt\")\n    doc2_tk = test_tokenizer(doc2, return_tensors=\"pt\")\n    #Find the embeddings for the doc\n    doc1_emb = get_mean_embed(doc1_tk)\n    doc2_emb = get_mean_embed(doc2_tk)\n    cos_sim = F.cosine_similarity(doc1_emb, doc2_emb)\n    return cos_sim.item()\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T16:01:11.208530Z","iopub.execute_input":"2025-08-23T16:01:11.208800Z","iopub.status.idle":"2025-08-23T16:01:11.213189Z","shell.execute_reply.started":"2025-08-23T16:01:11.208779Z","shell.execute_reply":"2025-08-23T16:01:11.212365Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"findsimilaritybetweendocs(\"What is Tarif\",\"Tariffs are taxes imposed by governments on imported goods and services, used to influence trade, protect domestic industries, and generate revenue.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T16:01:15.784267Z","iopub.execute_input":"2025-08-23T16:01:15.784749Z","iopub.status.idle":"2025-08-23T16:01:15.816779Z","shell.execute_reply.started":"2025-08-23T16:01:15.784727Z","shell.execute_reply":"2025-08-23T16:01:15.816193Z"}},"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"0.8751513957977295"},"metadata":{}}],"execution_count":76},{"cell_type":"code","source":"findsimilaritybetweendocs(\"What is the capital of France?\",\"The capital of France is Paris.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T16:01:54.231702Z","iopub.execute_input":"2025-08-23T16:01:54.231958Z","iopub.status.idle":"2025-08-23T16:01:54.256810Z","shell.execute_reply.started":"2025-08-23T16:01:54.231939Z","shell.execute_reply":"2025-08-23T16:01:54.256204Z"}},"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"0.9255729913711548"},"metadata":{}}],"execution_count":80},{"cell_type":"code","source":"findsimilaritybetweendocs(\"What is the capital of France?\",\"I gIood I\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T16:12:38.737867Z","iopub.execute_input":"2025-08-23T16:12:38.738169Z","iopub.status.idle":"2025-08-23T16:12:38.764510Z","shell.execute_reply.started":"2025-08-23T16:12:38.738124Z","shell.execute_reply":"2025-08-23T16:12:38.763911Z"}},"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"0.8300344347953796"},"metadata":{}}],"execution_count":86},{"cell_type":"code","source":"import json\nimport matplotlib.pyplot as plt\n\n# Path to your trainer_state.json (inside output_dir)\nstate_file = \"./checkpoints/trainer_state.json\"\n\nwith open(state_file, \"r\") as f:\n    trainer_state = json.load(f)\n\n# Extract loss and step history\nsteps = []\nlosses = []\n\nfor log in trainer_state.get(\"log_history\", []):\n    if \"loss\" in log:\n        steps.append(log[\"step\"])\n        losses.append(log[\"loss\"])\n\n# Plot\nplt.figure(figsize=(8, 5))\nplt.plot(steps, losses, marker=\"o\")\nplt.xlabel(\"Step\")\nplt.ylabel(\"Training Loss\")\nplt.title(\"Training Loss over Steps\")\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport matplotlib.pyplot as plt\n\n# Path to your trainer_state.json\nstate_file = \"./checkpoints/trainer_state.json\"\n\nwith open(state_file, \"r\") as f:\n    trainer_state = json.load(f)\n\n# Extract steps, loss, and learning rate\nsteps = []\nlosses = []\nlrs = []\n\nfor log in trainer_state.get(\"log_history\", []):\n    if \"loss\" in log and \"learning_rate\" in log:\n        steps.append(log[\"step\"])\n        losses.append(log[\"loss\"])\n        lrs.append(log[\"learning_rate\"])\n\n# Plot with dual axes\nfig, ax1 = plt.subplots(figsize=(9, 5))\n\ncolor = \"tab:blue\"\nax1.set_xlabel(\"Step\")\nax1.set_ylabel(\"Training Loss\", color=color)\nax1.plot(steps, losses, color=color, marker=\"o\", label=\"Loss\")\nax1.tick_params(axis=\"y\", labelcolor=color)\nax1.grid(True)\n\nax2 = ax1.twinx()  # second y-axis\ncolor = \"tab:red\"\nax2.set_ylabel(\"Learning Rate\", color=color)\nax2.plot(steps, lrs, color=color, linestyle=\"--\", label=\"Learning Rate\")\nax2.tick_params(axis=\"y\", labelcolor=color)\n\nfig.suptitle(\"Training Loss and Learning Rate over Steps\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}