{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12830809,"sourceType":"datasetVersion","datasetId":8114587}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset,concatenate_datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T13:03:10.290387Z","iopub.execute_input":"2025-08-22T13:03:10.290713Z","iopub.status.idle":"2025-08-22T13:03:16.036490Z","shell.execute_reply.started":"2025-08-22T13:03:10.290660Z","shell.execute_reply":"2025-08-22T13:03:16.035219Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"wiki_ds = load_dataset(\"csv\", data_files=\"/kaggle/input/wiki-cc-openweb/wiki_small_50k/wiki_small_50k.csv\")\nowt_ds = load_dataset(\"csv\", data_files=\"/kaggle/input/wiki-cc-openweb/openweb_50k (2)/openweb_50k.csv\")\ncc_news_ds = load_dataset(\"csv\", data_files=\"/kaggle/input/wiki-cc-openweb/cc_news/cc_news.csv\")\ntrain_ds = concatenate_datasets([wiki_ds[\"train\"],owt_ds[\"train\"],cc_news_ds[\"train\"]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T13:03:20.026953Z","iopub.execute_input":"2025-08-22T13:03:20.027633Z","iopub.status.idle":"2025-08-22T13:03:40.913509Z","shell.execute_reply.started":"2025-08-22T13:03:20.027591Z","shell.execute_reply":"2025-08-22T13:03:40.912433Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4a8265bc924496792ab1abefe8695d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f150f0fd54e443fa41098f74c8ede98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3527d78b8b9944218c2d8b64dda0711b"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"train_ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T13:05:03.988209Z","iopub.execute_input":"2025-08-22T13:05:03.988758Z","iopub.status.idle":"2025-08-22T13:05:03.996958Z","shell.execute_reply.started":"2025-08-22T13:05:03.988728Z","shell.execute_reply":"2025-08-22T13:05:03.995908Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text'],\n    num_rows: 170824\n})"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from tokenizers import ByteLevelBPETokenizer\nspecial_tokens = ['<s>','<pad>','</s>','<unk>','<mask>']\ntokenizer = ByteLevelBPETokenizer()\ntokenizer.train_from_iterator([x['text'] for x in train_ds], vocab_size = 10_000,min_frequency =2, special_tokens=special_tokens)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T14:45:05.601379Z","iopub.execute_input":"2025-08-22T14:45:05.601802Z","iopub.status.idle":"2025-08-22T14:47:35.114523Z","shell.execute_reply.started":"2025-08-22T14:45:05.601771Z","shell.execute_reply":"2025-08-22T14:47:35.113170Z"}},"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"}],"execution_count":131},{"cell_type":"code","source":"tokenizer.save_model(\"/kaggle/working/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T14:48:36.684627Z","iopub.execute_input":"2025-08-22T14:48:36.685114Z","iopub.status.idle":"2025-08-22T14:48:36.711952Z","shell.execute_reply.started":"2025-08-22T14:48:36.685084Z","shell.execute_reply":"2025-08-22T14:48:36.708914Z"}},"outputs":[{"execution_count":132,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/vocab.json', '/kaggle/working/merges.txt']"},"metadata":{}}],"execution_count":132},{"cell_type":"code","source":"from transformers import RobertaTokenizer\ntokenizer = RobertaTokenizer.from_pretrained(\"/kaggle/working/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T14:48:53.155391Z","iopub.execute_input":"2025-08-22T14:48:53.157108Z","iopub.status.idle":"2025-08-22T14:48:53.213995Z","shell.execute_reply.started":"2025-08-22T14:48:53.157056Z","shell.execute_reply":"2025-08-22T14:48:53.212306Z"}},"outputs":[],"execution_count":133},{"cell_type":"code","source":"train_ds['text'][1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T14:07:48.472349Z","iopub.execute_input":"2025-08-22T14:07:48.472746Z","iopub.status.idle":"2025-08-22T14:07:50.233419Z","shell.execute_reply.started":"2025-08-22T14:07:48.472714Z","shell.execute_reply":"2025-08-22T14:07:50.232018Z"}},"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"\"Cheryl S. McWatters Cheryl S. McWatters is professor and Father Edgar Thivierge Chair in Business History at the Telfer School of Management, University of Ottawa.\\n\\nEducation and career\\nShe was previously a professor at the University of Alberta and associate professor at McGill University. McWatters is a qualified accountant and earned her B.A., M.B.A. and Ph.D. all from Queen's University, Kingston, Ontario. Her work relates primarily to seventeenth and eighteenth century international trading networks. She is a trustee and former president of the Academy of Accounting Historians. McWatters established the Geraldine Grace and Maurice Alvin McWatters Visiting Fellowship at Queen's University in memory of her parents.\\n\\nEditing\\nMcWatters is editor of Accounting History Review and associate editor of the Journal of Operations Management and Accounting Perspectives. She serves on the editorial boards of Accounting, Auditing and Accountability Journal and Accounting Historians Journal.\\n\\nAwards\\nShingo Research Prize for Excellence in Manufacturing Research.\\n\\nSelected publications\\nHistoire des entreprises du transport: Évolutions comptables et managériales. Paris, France: L'Harmattan - Presses Universitaires de Sceaux, 2010. (Editor with H. Zimnovitch)\\nManagement Accounting: Analysis and Interpretation. Pearson Education, Harlow, 2008. (Editor with J.L. Zimmerman and D.C. Morse)\\n\\nReferences \\n\\nYear of birth missing (living people)\\nUniversity of Ottawa faculty\\nMcGill University faculty\\nQueen's University at Kingston alumni\\nCanadian accountants\\nLiving people\""},"metadata":{}}],"execution_count":64},{"cell_type":"code","source":"#Sample Text Data\n\n#from transformers import RobertaTokenizer\n#tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n\ntexts = [\n   \"Cheryl S. McWatters Cheryl S. McWatters is professor and Father Edgar Thivierge Chair in Business History at the Telfer School of Management, University of Ottawa.\\n\\nEducation and career\\nShe was previously a professor at the University of Alberta and associate professor at McGill University. McWatters is a qualified accountant and earned her B.A., M.B.A. and Ph.D. all from Queen's University, Kingston, Ontario. Her work relates primarily to seventeenth and eighteenth century international trading networks. She is a trustee and former president of the Academy of Accounting Historians. McWatters established the Geraldine Grace and Maurice Alvin McWatters Visiting Fellowship at Queen's University in memory of her parents.\\n\\nEditing\\nMcWatters is editor of Accounting History Review and associate editor of the Journal of Operations Management and Accounting Perspectives. She serves on the editorial boards of Accounting, Auditing and Accountability Journal and Accounting Historians Journal.\\n\\nAwards\\nShingo Research Prize for Excellence in Manufacturing Research.\\n\\nSelected publications\\nHistoire des entreprises du transport: Évolutions comptables et managériales. Paris, France: LHarmattan ''\",\n    \"This is a small test.\",\n    \"Roberta can learn from  examples.\",\n    \"Short text.\",\n    \"A slightly longer sentence for testing.\"\n]\n\nmax_length = 32  # truncate any sequence longer than this\n\n# Tokenize each text individually with truncation\ntrain_dataset = []\nfor text in texts:\n    enc = tokenizer(text, truncation=True, max_length=254,padding=\"max_length\", return_tensors=\"pt\")\n    train_dataset.append({\n        \"input_ids\": enc[\"input_ids\"].squeeze(0),\n        \"attention_mask\": enc[\"attention_mask\"].squeeze(0)\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T14:49:03.794630Z","iopub.execute_input":"2025-08-22T14:49:03.795833Z","iopub.status.idle":"2025-08-22T14:49:03.827233Z","shell.execute_reply.started":"2025-08-22T14:49:03.795785Z","shell.execute_reply":"2025-08-22T14:49:03.825279Z"}},"outputs":[],"execution_count":134},{"cell_type":"code","source":"# Real text\nfrom transformers import RobertaTokenizer\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\ntrain_dataset = []\nfor text in train_ds['text'][:5]:\n    enc = tokenizer(text, truncation=True, max_length=254, return_tensors=\"pt\")\n    train_dataset.append({\n        \"input_ids\": enc[\"input_ids\"].squeeze(0),\n        \"attention_mask\": enc[\"attention_mask\"].squeeze(0)\n    })\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T15:31:18.641052Z","iopub.execute_input":"2025-08-22T15:31:18.641524Z","iopub.status.idle":"2025-08-22T15:31:20.575235Z","shell.execute_reply.started":"2025-08-22T15:31:18.641495Z","shell.execute_reply":"2025-08-22T15:31:20.572936Z"}},"outputs":[],"execution_count":141},{"cell_type":"code","source":"train_dataset[0]['input_ids']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T15:28:51.063776Z","iopub.execute_input":"2025-08-22T15:28:51.065352Z","iopub.status.idle":"2025-08-22T15:28:51.081514Z","shell.execute_reply.started":"2025-08-22T15:28:51.065297Z","shell.execute_reply":"2025-08-22T15:28:51.079927Z"}},"outputs":[{"execution_count":139,"output_type":"execute_result","data":{"text/plain":"tensor([    0, 33531,   735,  3138,  2897,  7393,   735,  3138,    36,   844,\n          392,   504,  4156,   126,   316,   644, 34094,    43,    21,    41,\n         2370,  3551,   661,     4, 50118, 50118, 16431,   254, 50118,   894,\n         8069,    13,    65,    76,    19, 15437,   139, 13513, 13528,     6,\n           13,  2661,    37,  8273,  1182,  1720,    36, 16883,    39,   317,\n           11, 12858,    77,   373,  2115,   238,     8,    21,    39,  5548,\n        25209,     4,    91,   439,    15,     7,  6396,    23,     5,  2930,\n         3536,     9,  3920,     6,  2930,  1821,     9,  3920,     8,  1745,\n           18,  1821,     6,  6912,   131,    39,   521,  1165, 16876,  2575,\n         8876,     8, 10841, 15474,  7287,     6,    54,   258,  1059,  3615,\n         3059,    19,  7393,  1448,  6276,     4,    91,   702,    19, 24896,\n          661,  3351,  3889,  1488,   757,     6,     8,  4829,    20,   928,\n         6892,   139,    19, 24896,   661, 23790,  4061,  4616, 13238,     8,\n        23674,   661, 20205,   102, 22488,     4,    91, 13366, 13513, 13528,\n           18,  5133, 45940,     6,    19,  9622,    25,     7,   141,    39,\n          320,  3254,  6813,   106,     7,    28,   702,     4, 50118, 50118,\n        47380,  5678, 50118,  2897,   735,  3138, 50118,    20, 21232,   261,\n         7841,   139,     8,     5, 41172, 24636,    35,   504,   844,    12,\n        44328,    35,  4657,  3082,    93,    83, 11624,     9,  9149,   230,\n        10913, 14245,    15, 41172,  1223,  1741,   560,  1885,     8,    83,\n          990, 34680, 50118, 50118,  1366,  4156, 26906, 50118,  1646,  2022,\n         3257, 50118,   844,   212,    12, 11046, 15855,  8884, 50118, 26145,\n        33059,  2857,     9,     5,  2930,  3536,     9,  3920, 50118, 26145,\n        33059,  2857,     9,     5,  2930,  1821,     9,  3920, 50118, 35007,\n         3551,  1952,     2])"},"metadata":{}}],"execution_count":139},{"cell_type":"code","source":"from transformers import (\n    RobertaConfig,\n    RobertaForMaskedLM,\n    DataCollatorForLanguageModeling,\n    Trainer,\n    TrainingArguments,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T13:30:32.132191Z","iopub.execute_input":"2025-08-22T13:30:32.132589Z","iopub.status.idle":"2025-08-22T13:31:07.767300Z","shell.execute_reply.started":"2025-08-22T13:30:32.132562Z","shell.execute_reply":"2025-08-22T13:31:07.765946Z"}},"outputs":[{"name":"stderr","text":"2025-08-22 13:30:42.717891: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755869443.106792      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755869443.199402      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"data_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T14:49:13.859236Z","iopub.execute_input":"2025-08-22T14:49:13.860350Z","iopub.status.idle":"2025-08-22T14:49:13.868206Z","shell.execute_reply.started":"2025-08-22T14:49:13.860313Z","shell.execute_reply":"2025-08-22T14:49:13.866740Z"}},"outputs":[],"execution_count":135},{"cell_type":"code","source":"#for i in range(15):\n    #print(data_collator([train_dataset[i]]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T13:47:09.760456Z","iopub.execute_input":"2025-08-22T13:47:09.760903Z","iopub.status.idle":"2025-08-22T13:47:09.768030Z","shell.execute_reply.started":"2025-08-22T13:47:09.760871Z","shell.execute_reply":"2025-08-22T13:47:09.766793Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"config = RobertaConfig(\nvocab_size = tokenizer.vocab_size,\nhidden_size = 256, #Embedding size\nnum_hidden_layers = 6,#Number of encoder layers\nnum_attention_heads=4, # Attentions per encoder layer, divisible by 256\nintermediate_size = 1024, #FFN layer size\nmax_position_embeddings=256, #Position Embedding max size = embedding size\ntype_vocab_size =1,\nlayer_norm_eps = 1e-5,\ninitializer_range = 0.02,\npad_token_id = 1,\nbos_token_id = 0,\neos_token_id = 2\n)\nmodel = RobertaForMaskedLM(config)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T13:31:16.722189Z","iopub.execute_input":"2025-08-22T13:31:16.722599Z","iopub.status.idle":"2025-08-22T13:31:17.586129Z","shell.execute_reply.started":"2025-08-22T13:31:16.722571Z","shell.execute_reply":"2025-08-22T13:31:17.584825Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"tokenizer.vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T13:31:21.219645Z","iopub.execute_input":"2025-08-22T13:31:21.220134Z","iopub.status.idle":"2025-08-22T13:31:21.227826Z","shell.execute_reply.started":"2025-08-22T13:31:21.220105Z","shell.execute_reply":"2025-08-22T13:31:21.226220Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"50265"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# 4️⃣ Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./checkpoints\",\n    do_train=True,\n    save_steps=1,\n    logging_steps=1,\n    num_train_epochs=1,\n    per_device_train_batch_size=2,\n    fp16=False,\n    report_to=[]\n)\n\n# 5️⃣ Create Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    data_collator=data_collator\n)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T15:29:00.050760Z","iopub.execute_input":"2025-08-22T15:29:00.051929Z","iopub.status.idle":"2025-08-22T15:29:07.336040Z","shell.execute_reply.started":"2025-08-22T15:29:00.051795Z","shell.execute_reply":"2025-08-22T15:29:07.334638Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3/3 00:05, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>10.677200</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>10.795900</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>10.715800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":140,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3, training_loss=10.729628880818685, metrics={'train_runtime': 6.6352, 'train_samples_per_second': 0.754, 'train_steps_per_second': 0.452, 'total_flos': 36999984420.0, 'train_loss': 10.729628880818685, 'epoch': 1.0})"},"metadata":{}}],"execution_count":140},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T14:15:37.123285Z","iopub.execute_input":"2025-08-22T14:15:37.123679Z","iopub.status.idle":"2025-08-22T14:15:40.061970Z","shell.execute_reply.started":"2025-08-22T14:15:37.123652Z","shell.execute_reply":"2025-08-22T14:15:40.057756Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/3 : < :, Epoch 0.33/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/4032920361.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m                     )\n\u001b[1;32m   2554\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3744\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3745\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3747\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3808\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3810\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3811\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3812\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         outputs = self.roberta(\n\u001b[0m\u001b[1;32m   1088\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    803\u001b[0m                 \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m    806\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"absolute\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0membeddings\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index out of range in self"],"ename":"IndexError","evalue":"index out of range in self","output_type":"error"}],"execution_count":83},{"cell_type":"code","source":"config = RobertaConfig(\n\nvocab_size = tokenizer.vocab_size,\n\nhidden_size = 256, #Embedding size\n\nnum_hidden_layers = 6,#Number of encoder layers\n\nnum_attention_heads=4, # Attentions per encoder layer, divisible by 256\n\nintermediate_size = 1024, #FFN layer size\n\nmax_position_embeddings=256, #Position Embedding max size = embedding size\n\ntype_vocab_size =1,\n\nlayer_norm_eps = 1e-5,\n\ninitializer_range = 0.02,\n\npad_token_id = 1,\n\nbos_token_id = 0,\n\neos_token_id = 2\n\n)\n\nmodel = RobertaForMaskedLM(config)\n\n#model.resize_token_embeddings(len(tokenizer))\n\ntraining_args = TrainingArguments(\n\noutput_dir = \"./checkpoints/\",\ndo_train = True,\n\noverwrite_output_dir = False,\n\nnum_train_epochs=1,\n\nper_device_train_batch_size =16,\n\ngradient_accumulation_steps=2,\n\nlearning_rate = 5e-4,\n\nweight_decay = 0.01,\n\nwarmup_steps = 100,\n\nlogging_dir = \"./logs/\",\n\nlogging_steps = 1,\n\nsave_strategy=\"steps\",\n\nsave_steps =1,\n\nsave_total_limit=3,\n\nfp16= True,\n\ndataloader_num_workers=2,\n\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import logging\nlogging.set_verbosity_info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_ds,\n    data_collator=data_collator,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntry:\n    trainer.train()\nexcept Exception as e:\n    print(\"Trainer error:\", e)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nos.listdir(\"/kaggle/working/checkpoints\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available(), torch.cuda.device_count())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(tokenized_ds))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i, batch in enumerate(tokenized_ds):\n    print(i, batch)\n    if i > 5: break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\nimport torch\n\ntraining_args = TrainingArguments(\n    output_dir=\"./checkpoints\",\n    do_train=True,\n    save_steps=1,\n    logging_steps=1,\n    num_train_epochs=1,\n    per_device_train_batch_size=2,\n)\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n\n# Tiny dataset\ntrain_dataset = [{\"input_ids\": torch.tensor([101, 102, 103]), \"attention_mask\": torch.tensor([1,1,1]), \"labels\": torch.tensor(1)}] * 10\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset\n)\n\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaForMaskedLM, Trainer, TrainingArguments\nimport torch\n\n# tokenizer & model\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\nmodel = RobertaForMaskedLM.from_pretrained(\"roberta-base\")\n\n# texts\ntexts = [\n    \"Hello, I am a [MASK] model.\",\n    \"This is a small test [MASK].\",\n    \"Roberta can learn from [MASK] examples.\"\n] * 5\n\n# batch tokenize & pad\nencodings = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n\n# build dataset\ntrain_dataset = [\n    {\n        \"input_ids\": encodings[\"input_ids\"][i],\n        \"attention_mask\": encodings[\"attention_mask\"][i],\n        \"labels\": encodings[\"input_ids\"][i]\n    }\n    for i in range(len(texts))\n]\n\n# training args\ntraining_args = TrainingArguments(\n    output_dir=\"./checkpoints\",\n    do_train=True,\n    save_steps=1,\n    logging_steps=1,\n    num_train_epochs=1,\n    per_device_train_batch_size=2,\n    fp16=False,\n    report_to=[]\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset\n)\n\ntrainer.train()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n\n# 1️⃣ Load tokenizer and model\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\nmodel = RobertaForMaskedLM.from_pretrained(\"roberta-base\")\n\n# 2️⃣ Tiny toy dataset (variable-length sentences)\ntexts = [\n    \"Hello, I am a [MASK] model.\",\n    \"This is a small test [MASK].\",\n    \"Roberta can learn from [MASK] examples.\",\n    \"Short text.\",\n    \"A slightly longer sentence for testing.\"\n]\n\n# Tokenize each text individually without padding\ntrain_dataset = []\nfor text in texts:\n    enc = tokenizer(text, truncation=True, return_tensors=\"pt\")\n    train_dataset.append({\n        \"input_ids\": enc[\"input_ids\"].squeeze(0),\n        \"attention_mask\": enc[\"attention_mask\"].squeeze(0)\n    })\n\n# 3️⃣ Data collator for MLM\n# Automatically pads each batch and creates masked labels\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=True,\n    mlm_probability=0.15\n)\n\n# 4️⃣ Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./checkpoints\",\n    do_train=True,\n    save_steps=1,           # save every step\n    logging_steps=1,        # log every step\n    num_train_epochs=1,\n    per_device_train_batch_size=2,\n    fp16=False,             # safer for Kaggle\n    report_to=[]            # disable W&B\n)\n\n# 5️⃣ Create Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    data_collator=data_collator\n)\n\n# 6️⃣ Run training\ntrainer.train()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n\n# 1️⃣ Load tokenizer and model\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\nmodel = RobertaForMaskedLM.from_pretrained(\"roberta-base\")\n\n# 2️⃣ Tiny toy dataset (variable-length sentences)\ntexts = [\n    \"Hello, I am a [MASK] model.\",\n    \"This is a small test [MASK].\",\n    \"Roberta can learn from [MASK] examples.\",\n    \"Short text.\",\n    \"A slightly longer sentence for testing.\"\n]\n\nmax_length = 32  # truncate any sequence longer than this\n\n# Tokenize each text individually with truncation\ntrain_dataset = []\nfor text in texts:\n    enc = tokenizer(text, truncation=True, max_length=max_length, return_tensors=\"pt\")\n    train_dataset.append({\n        \"input_ids\": enc[\"input_ids\"].squeeze(0),\n        \"attention_mask\": enc[\"attention_mask\"].squeeze(0)\n    })\n\n# 3️⃣ Data collator for MLM\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=True,\n    mlm_probability=0.15\n)\n\n# 4️⃣ Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./checkpoints\",\n    do_train=True,\n    save_steps=1,\n    logging_steps=1,\n    num_train_epochs=1,\n    per_device_train_batch_size=2,\n    fp16=False,\n    report_to=[]\n)\n\n# 5️⃣ Create Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    data_collator=data_collator\n)\n\n# 6️⃣ Run training\ntrainer.train()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}